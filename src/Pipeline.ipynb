{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30HRRwSCAdKV"
   },
   "source": [
    "### Etapas\n",
    "1. Criação de features\n",
    "2. Tratamento de outliers\n",
    "3. Normalização dos dados\n",
    "4. Pipeline de pré-processamento\n",
    "5. Separação em treino/teste\n",
    "7. Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "q4F661DIPyJH"
   },
   "outputs": [],
   "source": [
    "# @title Importação das bibliotecas utilizadas no programa\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Carregamento do dataset\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Criação de features\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Normalização dos dados\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Treinamento do modelo \n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier,  AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import  AdaBoostClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 20320,
     "status": "error",
     "timestamp": 1762447144734,
     "user": {
      "displayName": "Pedro",
      "userId": "13797227125436573976"
     },
     "user_tz": 180
    },
    "id": "x-cs77oXAWpf",
    "outputId": "1e7a32a7-0679-4049-a587-db4439c1244b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathe\\AppData\\Local\\Temp\\ipykernel_18472\\4240293704.py:26: FutureWarning: The argument 'use_nullable_dtypes' is deprecated and will be removed in a future version.\n",
      "  wildfires = pd.read_parquet(\n"
     ]
    }
   ],
   "source": [
    "# @title Carregamento do dataset\n",
    "\n",
    "# Versão Google Collab\n",
    "'''\n",
    "github_link = \"https://github.com/mfigueireddo/ciencia-de-dados/blob/ba579573c5b8a9246ca04f7da29bc2c74c8b362c/datasets/pre-pipeline_wildfires.parquet\"\n",
    "url = github_link.replace(\"/blob/\", \"/raw/\")\n",
    "\n",
    "local_file_path = Path(\"/content/raw_wildfires.parquet\")\n",
    "\n",
    "# Faz uma requisição HTTP GET ao GitHub\n",
    "with requests.get(url, stream=True) as request:\n",
    "\n",
    "    request.raise_for_status() # Confere se houve êxito\n",
    "\n",
    "    with open(local_file_path , \"wb\") as file:\n",
    "        for chunk in request.iter_content(chunk_size=1024*1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "\n",
    "wildfires = pd.read_parquet(local_file_path,  engine=\"pyarrow\") # Leitura realizada com a engine pyarrow\n",
    "'''\n",
    "\n",
    "# Versão VSCode\n",
    "file_path = \"../datasets/pre-pipeline_wildfires.parquet\"\n",
    "\n",
    "wildfires = pd.read_parquet(\n",
    "    file_path,\n",
    "    engine=\"pyarrow\",\n",
    "    use_nullable_dtypes=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Variáveis globais\n",
    "\n",
    "data_column_name = 'data'\n",
    "id_column_name = 'fire_id'\n",
    "latitude_column_name = 'latitude'\n",
    "longitude_column_name = 'longitude'\n",
    "precipitation_column_name = 'precipitacao'\n",
    "max_temperature_column_name = 'temperatura_max'\n",
    "precipitation_sum_window_column_name = 'soma_precipitacao_90_dias'\n",
    "max_temperature_mean_column_name = 'media_temp_max_90_dias'\n",
    "season_column_name = 'estacao_ano_id'\n",
    "region_column_name = 'regiao_incendio'\n",
    "target_column_name = 'houve_incendio'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Criação de features\n",
    "<u>**Features criadas**</u>\n",
    "- Estação do ano\n",
    "- Região do incêndio\n",
    "- Soma da precipitação nos últimos 90 dias\n",
    "- Média de temperatura máxima nos últimos 90 dias\n",
    "\n",
    "**Observação 1**: as features estação do ano baseada e região do incêndio no momento não estão sendo utilizadas para treinar o modelo<br>\n",
    "**Observação 2**: caso essas duas features passem a ser utilizadas no treinamento do modelo, elas deverão ser tratadas porque são categóricas e não há relação numérica entre seus valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wUCqwXwyFR2O"
   },
   "outputs": [],
   "source": [
    "# @title Classe FeatureCreation\n",
    "\n",
    "class FeaturesCreation(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Colunas utilizadas\n",
    "        self.m_date_column = data_column_name\n",
    "        self.m_group_column = id_column_name\n",
    "        self.m_latitude_column = latitude_column_name\n",
    "        self.m_longitude_column = longitude_column_name\n",
    "        self.m_precipitation_column = precipitation_column_name \n",
    "        self.m_max_temperature_column = max_temperature_column_name \n",
    "\n",
    "        # Parâmetros personalizados para criação das features\n",
    "        self.m_precipitation_window_days = 90\n",
    "        self.m_max_temperature_window_days = 90\n",
    "\n",
    "        # Parâmetros do DBSCAN\n",
    "        self.mm_max_radiuskm = 1.0\n",
    "        self.m_min_samples = 5\n",
    "\n",
    "        # Objetos aprendidos no fit\n",
    "        self.m_dbscan = None\n",
    "        self.m_nearest_neighbors_core = None\n",
    "        self.m_core_labels = None\n",
    "        self.m_max_radius = None\n",
    "\n",
    "    # Conversão necessária para o DBSCAN\n",
    "    @staticmethod\n",
    "    def convert_to_radians(latitude_or_longitude):\n",
    "        return np.radians(latitude_or_longitude.astype(float))\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_month_to_season(month):\n",
    "        if month in (12, 1, 2): return 0  # Inverno\n",
    "        if month in (3, 4, 5): return 1  # Primavera\n",
    "        if month in (6, 7, 8): return 2  # Verão\n",
    "        return 3  # 9, 10, 11 -> Outono\n",
    "\n",
    "    def fit(self, dataframe, target=None):\n",
    "        dataframe = dataframe.copy()\n",
    "\n",
    "        # Desativa o DBSCAN caso não haja latitude e longitude\n",
    "        missing_cols = [column for column in [self.m_latitude_column, self.m_longitude_column] if column not in dataframe.columns]\n",
    "        if missing_cols:\n",
    "            self.m_dbscan = None\n",
    "            self.m_nearest_neighbors_core = None\n",
    "            self.m_core_labels = None\n",
    "            self.m_max_radius = None\n",
    "            return self\n",
    "\n",
    "        latitude_or_longitude = dataframe[[self.m_latitude_column, self.m_longitude_column]].to_numpy()\n",
    "        latitude_or_longitude_radians = self.convert_to_radians(latitude_or_longitude)\n",
    "\n",
    "        earth_radius = 6371.0\n",
    "        self.m_max_radius = self.mm_max_radiuskm / earth_radius\n",
    "\n",
    "        dbscan = DBSCAN(eps=self.m_max_radius, min_samples=self.m_min_samples, metric='haversine')\n",
    "        dbscan.fit(latitude_or_longitude_radians)\n",
    "        self.m_dbscan = dbscan\n",
    "\n",
    "        # Treina um NearestNeighbors apenas nos pontos-core para atribuição de novos pontos à clusters existentes em transform()\n",
    "        core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "        if hasattr(dbscan, 'core_sample_indices_') and len(dbscan.core_sample_indices_) > 0:\n",
    "            core_mask[dbscan.core_sample_indices_] = True\n",
    "            core_points = latitude_or_longitude_radians[core_mask]\n",
    "            core_labels = dbscan.labels_[core_mask]\n",
    "\n",
    "            if len(core_points) > 0:\n",
    "                nearest_neighbors = NearestNeighbors(n_neighbors=1, metric='haversine')\n",
    "                nearest_neighbors.fit(core_points)\n",
    "                self.m_nearest_neighbors_core = nearest_neighbors\n",
    "                self.m_core_labels = core_labels\n",
    "            else:\n",
    "                self.m_nearest_neighbors_core = None\n",
    "                self.m_core_labels = None\n",
    "        else:\n",
    "            self.m_nearest_neighbors_core = None\n",
    "            self.m_core_labels = None\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Rotula novos pontos\n",
    "    def assign_dbscanlabels(self, dataframe):\n",
    "\n",
    "        # Se não tivemos lat/lon ou DBSCAN treinado, devolve NaN\n",
    "        if self.m_nearest_neighbors_core is None or self.m_core_labels is None or self.m_max_radius is None:\n",
    "            return pd.Series([-1] * len(dataframe), index=dataframe.index, dtype='int64')\n",
    "\n",
    "        latitude_or_longitude = dataframe[[self.m_latitude_column, self.m_longitude_column]].to_numpy()\n",
    "        latitude_or_longitude_radians = self.convert_to_radians(latitude_or_longitude)\n",
    "\n",
    "        # Atribui rótulo do core mais próximo, desde que dentro do raio\n",
    "        distances, indices = self.m_nearest_neighbors_core.kneighbors(latitude_or_longitude_radians, n_neighbors=1, return_distance=True)\n",
    "        distances = distances.reshape(-1)\n",
    "        indices = indices.reshape(-1)\n",
    "\n",
    "        labels = np.full(len(dataframe), -1, dtype='int64')\n",
    "        within = distances <= self.m_max_radius\n",
    "        labels[within] = self.m_core_labels[indices[within]]\n",
    "\n",
    "        return pd.Series(labels, index=dataframe.index, dtype='int64')\n",
    "\n",
    "    # Adiciona médias móveis e soma pro grupo de incêndio\n",
    "    def add_temporal_rollings(self, dataframe):\n",
    "        \n",
    "        # Ordena por grupo e tempo para garantir rolling correto\n",
    "        if self.m_group_column in dataframe.columns and self.m_date_column in dataframe.columns:\n",
    "            dataframe = dataframe.sort_values([self.m_group_column, self.m_date_column])\n",
    "        else:\n",
    "            # Se faltar algo, só ordena por data (se existir)\n",
    "            if self.m_date_column in dataframe.columns:\n",
    "                dataframe = dataframe.sort_values(self.m_date_column)\n",
    "\n",
    "        # Rolling de precipitação (soma dos últimos 14 dias)\n",
    "        if self.m_precipitation_column in dataframe.columns:\n",
    "            dataframe[precipitation_sum_window_column_name] = (\n",
    "                dataframe.groupby(self.m_group_column, dropna=False)[self.m_precipitation_column]\n",
    "                  .rolling(self.m_precipitation_window_days, min_periods=1)\n",
    "                  .sum()\n",
    "                  .reset_index(level=0, drop=True)\n",
    "            )\n",
    "        else:\n",
    "            dataframe[precipitation_sum_window_column_name] = np.nan\n",
    "\n",
    "        # Rolling de temperatura máxima (média dos últimos 7 dias)\n",
    "        if self.m_max_temperature_column in dataframe.columns:\n",
    "            dataframe[max_temperature_mean_column_name] = (\n",
    "                dataframe.groupby(self.m_group_column, dropna=False)[self.m_max_temperature_column]\n",
    "                  .rolling(self.m_max_temperature_window_days, min_periods=1)\n",
    "                  .mean()\n",
    "                  .reset_index(level=0, drop=True)\n",
    "            )\n",
    "        else:\n",
    "            dataframe[max_temperature_mean_column_name] = np.nan\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    # Aplica transformações e cria as novas features\n",
    "    def transform(self, dataframe, target=None):\n",
    "        \n",
    "        # Trabalha em DataFrame para manter nomes/índices\n",
    "        dataframe = pd.DataFrame(dataframe).copy()\n",
    "\n",
    "        # Estação do ano\n",
    "        if self.m_date_column in dataframe.columns:\n",
    "            # Garante dtype datetime\n",
    "            dataframe[self.m_date_column] = pd.to_datetime(dataframe[self.m_date_column], errors='coerce')\n",
    "            estacao = dataframe[self.m_date_column].dt.month.map(self.convert_month_to_season).astype('Int64')\n",
    "            dataframe[season_column_name] = estacao.astype('float').astype('Int64')  # evita problemas de NaN -> imputar depois\n",
    "            dataframe[season_column_name] = dataframe[season_column_name].astype('float')\n",
    "        else:\n",
    "            dataframe[season_column_name] = np.nan\n",
    "\n",
    "        # Região \n",
    "        if all(c in dataframe.columns for c in [self.m_latitude_column, self.m_longitude_column]):\n",
    "            dataframe[region_column_name] = self.assign_dbscanlabels(dataframe).astype('int64')\n",
    "        else:\n",
    "            dataframe[region_column_name] = -1\n",
    "\n",
    "        # Rollings temporais\n",
    "        dataframe = self.add_temporal_rollings(dataframe)\n",
    "\n",
    "        '''\n",
    "        for col, dtype in dataframe.dtypes.items():\n",
    "            print(f\" - {col}: {dtype}\")\n",
    "        '''\n",
    "\n",
    "        return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXWH4spcYhkY"
   },
   "source": [
    "### 2. Tratamento de outliers\n",
    "\n",
    "**Transformação Logarítmica**\n",
    "- Aplica log(x) ou log(x+constante) para valores positivos\n",
    "- Muito eficaz para dados com distribuição assimétrica positiva\n",
    "- Comprime valores grandes e expande valores pequenos\n",
    "- Fórmula: X_log = log(X + c), onde c evita log(0)\n",
    "\n",
    "**Transformação Raiz Quadrada**\n",
    "- Menos drástica que a transformação logarítmica\n",
    "- Útil para dados de contagem e variáveis positivamente assimétricas\n",
    "- Fórmula: X_sqrt = sqrt(X)\n",
    "\n",
    "**Winsorização (Capping/Clipping)**\n",
    "\n",
    "A **Winsorização** é uma técnica de tratamento de outliers que **limita valores extremos** sem removê-los completamente. Em vez de excluir outliers, substituímos os valores extremos pelos valores de percentis específicos.\n",
    "\n",
    "**Como funciona:**\n",
    "- Define-se limites baseados em percentis (ex: 5º e 95º percentil)\n",
    "- Valores abaixo do limite inferior são substituídos pelo valor do limite inferior\n",
    "- Valores acima do limite superior são substituídos pelo valor do limite superior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmhrgtvXBUL8"
   },
   "source": [
    "| Variável                                 | Melhor método          | Resultado obtido |\n",
    "| :--------------------------------------- | :--------------------  | :-------------------------------------------------------------------------------------------------------- |\n",
    "| **precipitacao**                         | **Log(x + 1)**         | Assimetria (7.87 → 2.59)                                                                                  |\n",
    "| **umidade_relativa_max**                 | **Winsorização**       | Outliers (23 → 0)                                                                                         |\n",
    "| **umidade_relativa_min**                 | **Winsorização**       | Outliers (1155 → 0)                                                                                       |\n",
    "| **umidade_especifica**                   | **Sqrt**               | Simetria (0.89 → 0.16) / Outliers (6967 → 2102)                                                           |\n",
    "| **radiacao_solar**                       | **Sem transformação**  |                                                                                                           |\n",
    "| **temperatura_min**                      | **Winsorização**       | Outliers (5066 → 0)                                                                                       |\n",
    "| **temperatura_max**                      | **Winsorização**       | Outliers (5066 → 0)                                                                                       |\n",
    "| **velocidade_vento**                     | **Log(x + 1)**         | Assimetria (1.23 → 0.19) / Outliers (8723 → 1128)                                                         |\n",
    "| **indice_queima**                        | **Winsorização**       |                                                                                                           |\n",
    "| **umidade_combustivel_morto_100_horas**  | **Sqrt**               | Outliers (44 → 9)                                                                                         |\n",
    "| **umidade_combustivel_morto_1000_horas** | **Sqrt**               | Outliers (373 → 4)                                                                                        |\n",
    "| **componente_energia_lancada**           | **Sem transformação**  |                                                                                                           |\n",
    "| **evapotranspiracao_real**               | **Sqrt**               | Assimetria (0.71 → -0.00) / Outliers (3292 → 153)                                                         |\n",
    "| **evapotranspiracao_potencial**          | **Log(x + 1)**         | Outliers (679 → 0)                                                                                        |\n",
    "| **deficit_pressao_vapor**                | **Log(x + 1)**         | Outliers (14758 → 672)                                                                                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CYTi9RrGQd0x"
   },
   "outputs": [],
   "source": [
    "# @title Classe OutliersTreatment\n",
    "\n",
    "class OutliersTreatment(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.m_log_columns = [\n",
    "            \"precipitacao\",\n",
    "            \"velocidade_vento\",\n",
    "            \"evapotranspiracao_potencial\",\n",
    "            \"deficit_pressao_vapor\",\n",
    "        ]\n",
    "        self.m_sqrt_columns = [\n",
    "            \"umidade_especifica\",\n",
    "            \"umidade_combustivel_morto_100_horas\",\n",
    "            \"umidade_combustivel_morto_1000_horas\",\n",
    "            \"evapotranspiracao_real\",\n",
    "        ]\n",
    "        self.m_winsor_columns = [\n",
    "            \"umidade_relativa_max\",\n",
    "            \"umidade_relativa_min\",\n",
    "            \"temperatura_min\",\n",
    "            \"temperatura_max\",\n",
    "            \"indice_queima\",\n",
    "        ]\n",
    "        self.m_winsor_limits = (0.05, 0.05)\n",
    "\n",
    "    # Calcula parâmetros necessários para aplicar as transformações corretamente\n",
    "    def fit(self, dataframe, target=None):\n",
    "\n",
    "        # Garante que o usuário esteja enviado um dataframe no formato correto\n",
    "        dataframe = dataframe if isinstance(dataframe, pd.DataFrame) else pd.DataFrame(dataframe)\n",
    "\n",
    "        # Cálculo de offsets para garantir que não haverão valores zerados ou negativos\n",
    "        # \"coerce\" convete valores inválidos para NaN\n",
    "\n",
    "        self.m_log_offset = {}\n",
    "        for column in self.m_log_columns:\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                min_value = series.min()\n",
    "                self.m_log_offset[column] = (abs(min_value) + 1) if pd.notna(min_value) and min_value <= 0 else 1.0\n",
    "\n",
    "        self.m_sqrt_offset = {}\n",
    "        for column in self.m_sqrt_columns:\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                min_value = series.min()\n",
    "                self.m_sqrt_offset[column] = (abs(min_value) + 0.01) if pd.notna(min_value) and min_value < 0 else 0.0\n",
    "\n",
    "        # Garante que a winsorização só seja feita com colunas que realmente estão no dataframe\n",
    "        actual_columns_to_winsor = [column for column in self.m_winsor_columns if column in dataframe.columns]\n",
    "        low_quantile, high_quantile = self.m_winsor_limits\n",
    "\n",
    "        if actual_columns_to_winsor:\n",
    "            # Converte cada coluna para numérico (coerces -> NaN) e calcula quantis por coluna\n",
    "            winsor_dataframe = dataframe[actual_columns_to_winsor].apply(pd.to_numeric, errors=\"coerce\")\n",
    "            self.m_low_quantile  = winsor_dataframe.quantile(low_quantile)\n",
    "            self.m_high_quantile = winsor_dataframe.quantile(1 - high_quantile)\n",
    "        else:\n",
    "            # garante atributos vazios para não quebrar no transform()\n",
    "            self.m_low_quantile  = pd.Series(dtype=float)\n",
    "            self.m_high_quantile = pd.Series(dtype=float)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Aplica as transformações\n",
    "    def transform(self, dataframe):\n",
    "\n",
    "        # Garante que o usuário esteja enviado o dataframe correto\n",
    "        dataframe = dataframe.copy() if isinstance(dataframe, pd.DataFrame) else pd.DataFrame(dataframe).copy()\n",
    "\n",
    "        # LOG\n",
    "        for column, offset in self.m_log_offset.items():\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                dataframe[column] = np.log(series + offset)\n",
    "\n",
    "        # SQRT\n",
    "        for column, offset in self.m_sqrt_offset.items():\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                dataframe[column] = np.sqrt(series + offset)\n",
    "\n",
    "        # Winsorização\n",
    "        for column in self.m_low_quantile.index:\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                dataframe[column] = series.clip(lower=self.m_low_quantile[column], upper=self.m_high_quantile[column])\n",
    "\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normalização dos dados\n",
    "\n",
    "São utilizados para normalizar os dados numéricos\n",
    "- SimpleImputer\n",
    "- MinMaxScaler\n",
    "\n",
    "No momento, não estão sendo utilizadas features categóricas para treinar o modelo. Contudo, existe a implementação de uma normalização provisória para as mesmas utilizando\n",
    "- SimpleImputer\n",
    "- OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "id": "_HM_loh1QihZ"
   },
   "outputs": [],
   "source": [
    "# @title Classe DataNormalization\n",
    "\n",
    "'''\n",
    "categoric_cols = [season_column_name, region_column_name] \n",
    "\n",
    "def numeric_columns_selector(dataframe): \n",
    "    # Seleciona colunas numéricas, exceto as categóricas codificadas numericamente\n",
    "    num = dataframe.select_dtypes(include='number').columns.tolist() \n",
    "    return [column for column in num if column not in categoric_cols] \n",
    "\n",
    "categoric_columns_pipeline = Pipeline(steps=[ \n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), \n",
    "    ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False)), \n",
    "])\n",
    "'''\n",
    "\n",
    "numeric_columns_pipeline = Pipeline(steps=[ \n",
    "    ('imputer', SimpleImputer(strategy='median')), \n",
    "    ('scaler', MinMaxScaler()), \n",
    "    ]) \n",
    "\n",
    "DataNormalization = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical', numeric_columns_pipeline, make_column_selector(dtype_include='number')),\n",
    "        #('categoric', categoric_columns_pipeline, categoric_cols)\n",
    "    ],\n",
    "    remainder='passthrough'  # mantém quaisquer colunas não listadas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pipeline de pré-processamento\n",
    "\n",
    "Concentra 5 passos\n",
    "- Criação de features\n",
    "- Tratamento de outliers\n",
    "- Eliminação de features indesejadas no treinamento dos modelos\n",
    "- Normalização dos dados\n",
    "- Sanitização dos dados (garantindo que estão no formato esperado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Classe ColumnDropper\n",
    "\n",
    "# Elimina colunas não desejáveis no treinamento do modelo\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.columns = [data_column_name, id_column_name, latitude_column_name, longitude_column_name, season_column_name, region_column_name]\n",
    "    def fit(self, dataframe, target=None):\n",
    "        return self\n",
    "    def transform(self, dataframe):\n",
    "        return dataframe.drop(columns=self.columns, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Função sanitizer\n",
    "\n",
    "# Remove data e fire_id, além de converter o dataframe para o formato esperado pelos classificadores\n",
    "def sanitizer(features):\n",
    "    # Transforma em numpy.ndarray\n",
    "    if isinstance(features, pd.DataFrame):\n",
    "        features = features.select_dtypes(include='number')  \n",
    "    return features   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "kyUKC9duPwYA"
   },
   "outputs": [],
   "source": [
    "# @title Pipeline preprocess\n",
    "\n",
    "preprocess = Pipeline(steps=[\n",
    "    (\"features_creation\", FeaturesCreation()),\n",
    "    (\"outliers_treatment\", OutliersTreatment()),\n",
    "    (\"drop_unused_columns\", ColumnDropper()),\n",
    "    (\"data_normalization\", DataNormalization),\n",
    "    (\"sanitize\", FunctionTransformer(sanitizer, validate=False, feature_names_out='one-to-one'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSxExLXsXrdc"
   },
   "source": [
    "### 5. Separação em treino/teste\n",
    "\n",
    "**Time Series Cross Validation**\n",
    "\n",
    "A Time Series Cross Validation é uma técnica especializada para validar modelos quando os dados possuem ordem cronológica. Diferente das técnicas tradicionais, ela respeita a estrutura temporal dos dados.\n",
    "\n",
    "Foi implemetada uma função personalizada para que dados referentes ao mesmo incêndio permanecessem nos mesmos grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "form",
    "id": "4l1XXAgXQkt7"
   },
   "outputs": [],
   "source": [
    "# @title Função group_time_series_cross_validation\n",
    "\n",
    "# Gera folds (train_idx, test_idx) respeitando ordem temporal por grupo, embargo em nível de grupo e exclusão mútua treino/teste por grupo.\n",
    "def group_time_series_cross_validation():\n",
    "    dataframe = wildfires\n",
    "    time_column = data_column_name\n",
    "    group_column = id_column_name\n",
    "    folds_amount = 5\n",
    "    fold_groups_size = 1\n",
    "    gap_between_groups_amount = 0\n",
    "\n",
    "    # Ordena grupos pelo primeiro timestamp\n",
    "    first_time = (\n",
    "        dataframe[[group_column, time_column]]\n",
    "        .dropna(subset=[time_column])\n",
    "        .groupby(group_column)[time_column]\n",
    "        .min()\n",
    "        .sort_values()\n",
    "    )\n",
    "    ordered_groups = first_time.index.to_numpy()\n",
    "    ordered_groups_len = len(ordered_groups)\n",
    "\n",
    "    groups_amount_by_step = fold_groups_size\n",
    "\n",
    "    min_train_groups = max(1, fold_groups_size) # pelo menos 1\n",
    "\n",
    "    # Âncora: último grupo incluso no treino\n",
    "    # Precisamos garantir espaço para gap + teste à frente\n",
    "    max_anchor = ordered_groups_len - gap_between_groups_amount - fold_groups_size\n",
    "    if max_anchor <= min_train_groups:\n",
    "        return  # não há splits possíveis\n",
    "\n",
    "    splits = 0\n",
    "    anchor = min_train_groups\n",
    "    while anchor <= max_anchor and splits < folds_amount:\n",
    "        train_groups = ordered_groups[:anchor]\n",
    "\n",
    "        test_start = anchor + gap_between_groups_amount\n",
    "        test_end = test_start + fold_groups_size\n",
    "        test_groups = ordered_groups[test_start:test_end]\n",
    "\n",
    "        train_idx = dataframe.index[dataframe[group_column].isin(train_groups)].to_numpy()\n",
    "        test_idx  = dataframe.index[dataframe[group_column].isin(test_groups)].to_numpy()\n",
    "\n",
    "        if train_idx.size and test_idx.size:\n",
    "            yield (train_idx, test_idx)\n",
    "            splits += 1\n",
    "\n",
    "        anchor += groups_amount_by_step\n",
    "\n",
    "cross_validation = cross_validation_splits = list(group_time_series_cross_validation())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Treinamento dos modelos\n",
    "\n",
    "Os modelos treinados foram\n",
    "- Dummy (mais frequente)\n",
    "- Regressão Logística\n",
    "- Árvore de Decisão\n",
    "- Random Forest\n",
    "- Naive Bayes\n",
    "- KNN\n",
    "- Gradient Boosting **(NOVO)**\n",
    "- AdaBoost **(NOVO)**\n",
    "- HistGradientBoosting **(NOVO)**\n",
    "- RidgeClassifier **(NOVO)**\n",
    "- XGBoost **(NOVO)**\n",
    "- CatBoost **(NOVO)**\n",
    "- LightGBM **(NOVO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Escolha dos modelos e das métricas\n",
    "\n",
    "modelos = {\n",
    "    \"Dummy (mais frequente)\": DummyClassifier(strategy=\"most_frequent\"), # Baseline simples\n",
    "    \"Regressão Logística\": LogisticRegression(C=0.5, penalty='l2', solver='liblinear', max_iter=2000), # Regularização L2 leve\n",
    "    \"Árvore de Decisão\": DecisionTreeClassifier(max_depth=8, min_samples_split=4, min_samples_leaf=2, random_state=42), # Controle de profundidade e tamanho da folha\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=150, max_depth=10, min_samples_split=5,random_state=42, n_jobs=-1), # Mais árvores e profundidade moderada\n",
    "    \"Naive Bayes\": GaussianNB(var_smoothing=1e-8), # Suavização leve (mais estável)\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=7, weights='distance'), # Mais vizinhos e distância ponderada\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42), # Parâmetros leves conforme aula\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=42), # Taxa de aprendizado menor\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(max_iter=150, learning_rate=0.05, max_depth=5, random_state=42), # Mais iterações e taxa menor\n",
    "    \"RidgeClassifier\": RidgeClassifier(alpha=1.0), # Regularização leve\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', n_estimators=150, learning_rate=0.05, max_depth=5, subsample=0.8, colsample_bytree=0.8, random_state=42), #  Parâmetros típicos de equilíbrio (aula 15 parte 4)\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=150, learning_rate=0.05, depth=5,verbose=0, random_state=42), # Taxa de aprendizado reduzida e iterações extras\n",
    "    # \"LightGBM\": LGBMClassifier(n_estimators=150, learning_rate=0.05, num_leaves=31, max_depth=6, random_state=42) # Profundidade controlada e taxa moderada\n",
    "}\n",
    "\n",
    "# Métricas de avaliação\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': make_scorer(recall_score, zero_division=0),\n",
    "    'f1': make_scorer(f1_score, zero_division=0),\n",
    "    'roc_auc': 'roc_auc' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando modelo: Dummy (mais frequente)...\n",
      " Modelo Dummy (mais frequente) treinado com sucesso.\n",
      "\n",
      "Treinando modelo: Regressão Logística...\n",
      " Modelo Regressão Logística treinado com sucesso.\n",
      "\n",
      "Treinando modelo: Árvore de Decisão...\n",
      " Modelo Árvore de Decisão treinado com sucesso.\n",
      "\n",
      "Treinando modelo: Random Forest...\n",
      " Modelo Random Forest treinado com sucesso.\n",
      "\n",
      "Treinando modelo: Naive Bayes...\n",
      " Modelo Naive Bayes treinado com sucesso.\n",
      "\n",
      "Treinando modelo: KNN...\n",
      " Modelo KNN treinado com sucesso.\n",
      "\n",
      "Treinando modelo: Gradient Boosting...\n",
      " Modelo Gradient Boosting treinado com sucesso.\n",
      "\n",
      "Treinando modelo: AdaBoost...\n",
      " Modelo AdaBoost treinado com sucesso.\n",
      "\n",
      "Treinando modelo: HistGradientBoosting...\n",
      " Modelo HistGradientBoosting treinado com sucesso.\n",
      "\n",
      "Treinando modelo: RidgeClassifier...\n",
      " Modelo RidgeClassifier treinado com sucesso.\n",
      "\n",
      "Treinando modelo: XGBoost...\n",
      " Modelo XGBoost treinado com sucesso.\n",
      "\n",
      "Treinando modelo: CatBoost...\n",
      " Modelo CatBoost treinado com sucesso.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.934667</td>\n",
       "      <td>0.830435</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.635167</td>\n",
       "      <td>0.949630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.724856</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.570613</td>\n",
       "      <td>0.953679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HistGradientBoosting</td>\n",
       "      <td>0.922667</td>\n",
       "      <td>0.628235</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.516660</td>\n",
       "      <td>0.968049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.511077</td>\n",
       "      <td>0.873333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.925333</td>\n",
       "      <td>0.859274</td>\n",
       "      <td>0.506667</td>\n",
       "      <td>0.497508</td>\n",
       "      <td>0.946963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.779464</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.493215</td>\n",
       "      <td>0.946568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.905333</td>\n",
       "      <td>0.773950</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.486567</td>\n",
       "      <td>0.895358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.921333</td>\n",
       "      <td>0.581263</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.466078</td>\n",
       "      <td>0.830222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Árvore de Decisão</td>\n",
       "      <td>0.909333</td>\n",
       "      <td>0.506298</td>\n",
       "      <td>0.413333</td>\n",
       "      <td>0.390672</td>\n",
       "      <td>0.727062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RidgeClassifier</td>\n",
       "      <td>0.922667</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.357059</td>\n",
       "      <td>0.993284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regressão Logística</td>\n",
       "      <td>0.921333</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.334559</td>\n",
       "      <td>0.974914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dummy (mais frequente)</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Modelo  Accuracy  Precision    Recall  F1-score   ROC AUC\n",
       "10                 XGBoost  0.934667   0.830435  0.653333  0.635167  0.949630\n",
       "4              Naive Bayes  0.908000   0.724856  0.653333  0.570613  0.953679\n",
       "8     HistGradientBoosting  0.922667   0.628235  0.560000  0.516660  0.968049\n",
       "5                      KNN  0.926667   0.900000  0.453333  0.511077  0.873333\n",
       "3            Random Forest  0.925333   0.859274  0.506667  0.497508  0.946963\n",
       "11                CatBoost  0.906667   0.779464  0.560000  0.493215  0.946568\n",
       "7                 AdaBoost  0.905333   0.773950  0.573333  0.486567  0.895358\n",
       "6        Gradient Boosting  0.921333   0.581263  0.493333  0.466078  0.830222\n",
       "2        Árvore de Decisão  0.909333   0.506298  0.413333  0.390672  0.727062\n",
       "9          RidgeClassifier  0.922667   0.520000  0.360000  0.357059  0.993284\n",
       "1      Regressão Logística  0.921333   0.529412  0.293333  0.334559  0.974914\n",
       "0   Dummy (mais frequente)  0.900000   0.000000  0.000000  0.000000  0.500000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Algoritmo de treinamento\n",
    "\n",
    "features = wildfires.drop(columns=target_column_name)\n",
    "target = wildfires[target_column_name].astype(int)\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for nome, modelo in modelos.items():\n",
    "    print(f\"Treinando modelo: {nome}...\")\n",
    "\n",
    "    try:\n",
    "        pipeline = Pipeline([\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"nan_shield\", SimpleImputer(strategy=\"constant\", fill_value=0.0)), # Blindagem contra NaN\n",
    "            (\"classificator\", modelo)\n",
    "        ])\n",
    "\n",
    "        scores = cross_validate(pipeline, features, target, cv=cross_validation, scoring=scoring, n_jobs=1)\n",
    "\n",
    "        resultados.append({\n",
    "            \"Modelo\": nome,\n",
    "            \"Accuracy\": np.mean(scores['test_accuracy']),\n",
    "            \"Precision\": np.mean(scores['test_precision']),\n",
    "            \"Recall\": np.mean(scores['test_recall']),\n",
    "            \"F1-score\": np.mean(scores['test_f1']),\n",
    "            \"ROC AUC\": np.mean(scores['test_roc_auc']),\n",
    "        })\n",
    "\n",
    "        print(f\" Modelo {nome} treinado com sucesso.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao rodar o modelo {nome}: {e}\\n\")\n",
    "\n",
    "# Mostra os resultados\n",
    "df_resultados = pd.DataFrame(resultados).sort_values(\"F1-score\", ascending=False)\n",
    "df_resultados"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
