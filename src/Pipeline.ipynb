{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30HRRwSCAdKV"
   },
   "source": [
    "### Etapas\n",
    "1. Cria√ß√£o de features\n",
    "2. Tratamento de outliers\n",
    "3. Normaliza√ß√£o dos dados\n",
    "4. Pipeline de pr√©-processamento\n",
    "5. Separa√ß√£o em treino/teste\n",
    "6. Balanceamento dos dados\n",
    "7. Treinamento dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4F661DIPyJH"
   },
   "outputs": [],
   "source": [
    "# @title Importa√ß√£o das bibliotecas utilizadas no programa\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Carregamento do dataset\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Cria√ß√£o de features\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Normaliza√ß√£o dos dados\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Treinamento do modelo \n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier,  AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import  AdaBoostClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 20320,
     "status": "error",
     "timestamp": 1762447144734,
     "user": {
      "displayName": "Pedro",
      "userId": "13797227125436573976"
     },
     "user_tz": 180
    },
    "id": "x-cs77oXAWpf",
    "outputId": "1e7a32a7-0679-4049-a587-db4439c1244b"
   },
   "outputs": [],
   "source": [
    "# @title Carregamento do dataset\n",
    "\n",
    "# Vers√£o Google Collab\n",
    "# github_link = \"https://github.com/mfigueireddo/ciencia-de-dados/blob/ba579573c5b8a9246ca04f7da29bc2c74c8b362c/datasets/pre-pipeline_wildfires.parquet\"\n",
    "# url = github_link.replace(\"/blob/\", \"/raw/\")\n",
    "\n",
    "# local_file_path = Path(\"/content/raw_wildfires.parquet\")\n",
    "\n",
    "# # Faz uma requisi√ß√£o HTTP GET ao GitHub\n",
    "# with requests.get(url, stream=True) as request:\n",
    "\n",
    "#     request.raise_for_status() # Confere se houve √™xito\n",
    "\n",
    "#     with open(local_file_path , \"wb\") as file:\n",
    "#         for chunk in request.iter_content(chunk_size=1024*1024):\n",
    "#             if chunk:\n",
    "#                 file.write(chunk)\n",
    "\n",
    "# wildfires = pd.read_parquet(local_file_path,  engine=\"pyarrow\") # Leitura realizada com a engine pyarrow\n",
    "\n",
    "\n",
    "# Vers√£o VSCode\n",
    "file_path = \"../datasets/pre-pipeline_wildfires.parquet\"\n",
    "\n",
    "wildfires = pd.read_parquet(\n",
    "    file_path,\n",
    "    engine=\"pyarrow\",\n",
    "    use_nullable_dtypes=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Vari√°veis globais\n",
    "\n",
    "data_column_name = 'data'\n",
    "id_column_name = 'fire_id'\n",
    "latitude_column_name = 'latitude'\n",
    "longitude_column_name = 'longitude'\n",
    "precipitation_column_name = 'precipitacao'\n",
    "max_temperature_column_name = 'temperatura_max'\n",
    "precipitation_sum_window_column_name = 'soma_precipitacao_90_dias'\n",
    "max_temperature_mean_column_name = 'media_temp_max_90_dias'\n",
    "season_column_name = 'estacao_ano_id'\n",
    "region_column_name = 'regiao_incendio'\n",
    "target_column_name = 'houve_incendio'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cria√ß√£o de features\n",
    "<u>**Features criadas**</u>\n",
    "- Esta√ß√£o do ano\n",
    "- Regi√£o do inc√™ndio\n",
    "- Soma da precipita√ß√£o nos √∫ltimos 90 dias\n",
    "- M√©dia de temperatura m√°xima nos √∫ltimos 90 dias\n",
    "\n",
    "**Observa√ß√£o 1**: as features esta√ß√£o do ano baseada e regi√£o do inc√™ndio no momento n√£o est√£o sendo utilizadas para treinar o modelo<br>\n",
    "**Observa√ß√£o 2**: caso essas duas features passem a ser utilizadas no treinamento do modelo, elas dever√£o ser tratadas porque s√£o categ√≥ricas e n√£o h√° rela√ß√£o num√©rica entre seus valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUCqwXwyFR2O"
   },
   "outputs": [],
   "source": [
    "# @title Classe FeatureCreation\n",
    "\n",
    "window_days = 90\n",
    "\n",
    "class FeaturesCreation(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Colunas utilizadas\n",
    "        self.m_date_column = data_column_name\n",
    "        self.m_group_column = id_column_name\n",
    "        self.m_latitude_column = latitude_column_name\n",
    "        self.m_longitude_column = longitude_column_name\n",
    "        self.m_precipitation_column = precipitation_column_name \n",
    "        self.m_max_temperature_column = max_temperature_column_name \n",
    "\n",
    "        # Par√¢metros personalizados para cria√ß√£o das features\n",
    "        self.m_precipitation_window_days = window_days\n",
    "        self.m_max_temperature_window_days = window_days\n",
    "\n",
    "        # Par√¢metros do DBSCAN\n",
    "        self.mm_max_radiuskm = 1.0\n",
    "        self.m_min_samples = 5\n",
    "\n",
    "        # Objetos aprendidos no fit\n",
    "        self.m_dbscan = None\n",
    "        self.m_nearest_neighbors_core = None\n",
    "        self.m_core_labels = None\n",
    "        self.m_max_radius = None\n",
    "\n",
    "    # Convers√£o necess√°ria para o DBSCAN\n",
    "    @staticmethod\n",
    "    def convert_to_radians(latitude_or_longitude):\n",
    "        return np.radians(latitude_or_longitude.astype(float))\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_month_to_season(month):\n",
    "        if month in (12, 1, 2): return 0  # Inverno\n",
    "        if month in (3, 4, 5): return 1  # Primavera\n",
    "        if month in (6, 7, 8): return 2  # Ver√£o\n",
    "        return 3  # 9, 10, 11 -> Outono\n",
    "\n",
    "    def fit(self, dataframe, target=None):\n",
    "        dataframe = dataframe.copy()\n",
    "\n",
    "        # Desativa o DBSCAN caso n√£o haja latitude e longitude\n",
    "        missing_cols = [column for column in [self.m_latitude_column, self.m_longitude_column] if column not in dataframe.columns]\n",
    "        if missing_cols:\n",
    "            self.m_dbscan = None\n",
    "            self.m_nearest_neighbors_core = None\n",
    "            self.m_core_labels = None\n",
    "            self.m_max_radius = None\n",
    "            return self\n",
    "\n",
    "        latitude_or_longitude = dataframe[[self.m_latitude_column, self.m_longitude_column]].to_numpy()\n",
    "        latitude_or_longitude_radians = self.convert_to_radians(latitude_or_longitude)\n",
    "\n",
    "        earth_radius = 6371.0\n",
    "        self.m_max_radius = self.mm_max_radiuskm / earth_radius\n",
    "\n",
    "        dbscan = DBSCAN(eps=self.m_max_radius, min_samples=self.m_min_samples, metric='haversine')\n",
    "        dbscan.fit(latitude_or_longitude_radians)\n",
    "        self.m_dbscan = dbscan\n",
    "\n",
    "        # Treina um NearestNeighbors apenas nos pontos-core para atribui√ß√£o de novos pontos √† clusters existentes em transform()\n",
    "        core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "        if hasattr(dbscan, 'core_sample_indices_') and len(dbscan.core_sample_indices_) > 0:\n",
    "            core_mask[dbscan.core_sample_indices_] = True\n",
    "            core_points = latitude_or_longitude_radians[core_mask]\n",
    "            core_labels = dbscan.labels_[core_mask]\n",
    "\n",
    "            if len(core_points) > 0:\n",
    "                nearest_neighbors = NearestNeighbors(n_neighbors=1, metric='haversine')\n",
    "                nearest_neighbors.fit(core_points)\n",
    "                self.m_nearest_neighbors_core = nearest_neighbors\n",
    "                self.m_core_labels = core_labels\n",
    "            else:\n",
    "                self.m_nearest_neighbors_core = None\n",
    "                self.m_core_labels = None\n",
    "        else:\n",
    "            self.m_nearest_neighbors_core = None\n",
    "            self.m_core_labels = None\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Rotula novos pontos\n",
    "    def assign_dbscanlabels(self, dataframe):\n",
    "\n",
    "        # Se n√£o tivemos lat/lon ou DBSCAN treinado, devolve NaN\n",
    "        if self.m_nearest_neighbors_core is None or self.m_core_labels is None or self.m_max_radius is None:\n",
    "            return pd.Series([-1] * len(dataframe), index=dataframe.index, dtype='int64')\n",
    "\n",
    "        latitude_or_longitude = dataframe[[self.m_latitude_column, self.m_longitude_column]].to_numpy()\n",
    "        latitude_or_longitude_radians = self.convert_to_radians(latitude_or_longitude)\n",
    "\n",
    "        # Atribui r√≥tulo do core mais pr√≥ximo, desde que dentro do raio\n",
    "        distances, indices = self.m_nearest_neighbors_core.kneighbors(latitude_or_longitude_radians, n_neighbors=1, return_distance=True)\n",
    "        distances = distances.reshape(-1)\n",
    "        indices = indices.reshape(-1)\n",
    "\n",
    "        labels = np.full(len(dataframe), -1, dtype='int64')\n",
    "        within = distances <= self.m_max_radius\n",
    "        labels[within] = self.m_core_labels[indices[within]]\n",
    "\n",
    "        return pd.Series(labels, index=dataframe.index, dtype='int64')\n",
    "\n",
    "    # Adiciona m√©dias m√≥veis e soma pro grupo de inc√™ndio\n",
    "    def add_temporal_rollings(self, dataframe):\n",
    "        \n",
    "        # Ordena por grupo e tempo para garantir rolling correto\n",
    "        if self.m_group_column in dataframe.columns and self.m_date_column in dataframe.columns:\n",
    "            dataframe = dataframe.sort_values([self.m_group_column, self.m_date_column])\n",
    "        else:\n",
    "            # Se faltar algo, s√≥ ordena por data (se existir)\n",
    "            if self.m_date_column in dataframe.columns:\n",
    "                dataframe = dataframe.sort_values(self.m_date_column)\n",
    "\n",
    "        # Rolling de precipita√ß√£o (soma dos √∫ltimos 14 dias)\n",
    "        if self.m_precipitation_column in dataframe.columns:\n",
    "            dataframe[precipitation_sum_window_column_name] = (\n",
    "                dataframe.groupby(self.m_group_column, dropna=False)[self.m_precipitation_column]\n",
    "                  .rolling(self.m_precipitation_window_days, min_periods=1)\n",
    "                  .sum()\n",
    "                  .reset_index(level=0, drop=True)\n",
    "            )\n",
    "        else:\n",
    "            dataframe[precipitation_sum_window_column_name] = np.nan\n",
    "\n",
    "        # Rolling de temperatura m√°xima (m√©dia dos √∫ltimos 7 dias)\n",
    "        if self.m_max_temperature_column in dataframe.columns:\n",
    "            dataframe[max_temperature_mean_column_name] = (\n",
    "                dataframe.groupby(self.m_group_column, dropna=False)[self.m_max_temperature_column]\n",
    "                  .rolling(self.m_max_temperature_window_days, min_periods=1)\n",
    "                  .mean()\n",
    "                  .reset_index(level=0, drop=True)\n",
    "            )\n",
    "        else:\n",
    "            dataframe[max_temperature_mean_column_name] = np.nan\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    # Aplica transforma√ß√µes e cria as novas features\n",
    "    def transform(self, dataframe, target=None):\n",
    "        \n",
    "        # Trabalha em DataFrame para manter nomes/√≠ndices\n",
    "        dataframe = pd.DataFrame(dataframe).copy()\n",
    "\n",
    "        # Esta√ß√£o do ano\n",
    "        if self.m_date_column in dataframe.columns:\n",
    "            # Garante dtype datetime\n",
    "            dataframe[self.m_date_column] = pd.to_datetime(dataframe[self.m_date_column], errors='coerce')\n",
    "            estacao = dataframe[self.m_date_column].dt.month.map(self.convert_month_to_season).astype('Int64')\n",
    "            dataframe[season_column_name] = estacao.astype('float').astype('Int64')  # evita problemas de NaN -> imputar depois\n",
    "            dataframe[season_column_name] = dataframe[season_column_name].astype('float')\n",
    "        else:\n",
    "            dataframe[season_column_name] = np.nan\n",
    "\n",
    "        # Regi√£o \n",
    "        if all(c in dataframe.columns for c in [self.m_latitude_column, self.m_longitude_column]):\n",
    "            dataframe[region_column_name] = self.assign_dbscanlabels(dataframe).astype('int64')\n",
    "        else:\n",
    "            dataframe[region_column_name] = -1\n",
    "\n",
    "        # Rollings temporais\n",
    "        dataframe = self.add_temporal_rollings(dataframe)\n",
    "\n",
    "        '''\n",
    "        for col, dtype in dataframe.dtypes.items():\n",
    "            print(f\" - {col}: {dtype}\")\n",
    "        '''\n",
    "\n",
    "        return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXWH4spcYhkY"
   },
   "source": [
    "### 2. Tratamento de outliers\n",
    "\n",
    "<u>**Transforma√ß√£o Logar√≠tmica**</u>\n",
    "- Aplica log(x) ou log(x+constante) para valores positivos\n",
    "- Muito eficaz para dados com distribui√ß√£o assim√©trica positiva\n",
    "- Comprime valores grandes e expande valores pequenos\n",
    "- F√≥rmula: X_log = log(X + c), onde c evita log(0)\n",
    "\n",
    "<u>**Transforma√ß√£o Raiz Quadrada**</u>\n",
    "- Menos dr√°stica que a transforma√ß√£o logar√≠tmica\n",
    "- √ötil para dados de contagem e vari√°veis positivamente assim√©tricas\n",
    "- F√≥rmula: X_sqrt = sqrt(X)\n",
    "\n",
    "<u>**Winsoriza√ß√£o (Capping/Clipping)**</u>\n",
    "\n",
    "A **Winsoriza√ß√£o** √© uma t√©cnica de tratamento de outliers que **limita valores extremos** sem remov√™-los completamente. Em vez de excluir outliers, substitu√≠mos os valores extremos pelos valores de percentis espec√≠ficos.\n",
    "\n",
    "**Como funciona:**\n",
    "- Define-se limites baseados em percentis (ex: 5¬∫ e 95¬∫ percentil)\n",
    "- Valores abaixo do limite inferior s√£o substitu√≠dos pelo valor do limite inferior\n",
    "- Valores acima do limite superior s√£o substitu√≠dos pelo valor do limite superior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmhrgtvXBUL8"
   },
   "source": [
    "| Vari√°vel                                 | Melhor m√©todo          | Resultado obtido |\n",
    "| :--------------------------------------- | :--------------------  | :-------------------------------------------------------------------------------------------------------- |\n",
    "| **precipitacao**                         | **Log(x + 1)**         | Assimetria (7.87 ‚Üí 2.59)                                                                                  |\n",
    "| **umidade_relativa_max**                 | **Winsoriza√ß√£o**       | Outliers (23 ‚Üí 0)                                                                                         |\n",
    "| **umidade_relativa_min**                 | **Winsoriza√ß√£o**       | Outliers (1155 ‚Üí 0)                                                                                       |\n",
    "| **umidade_especifica**                   | **Sqrt**               | Simetria (0.89 ‚Üí 0.16) / Outliers (6967 ‚Üí 2102)                                                           |\n",
    "| **radiacao_solar**                       | **Sem transforma√ß√£o**  |                                                                                                           |\n",
    "| **temperatura_min**                      | **Winsoriza√ß√£o**       | Outliers (5066 ‚Üí 0)                                                                                       |\n",
    "| **temperatura_max**                      | **Winsoriza√ß√£o**       | Outliers (5066 ‚Üí 0)                                                                                       |\n",
    "| **velocidade_vento**                     | **Log(x + 1)**         | Assimetria (1.23 ‚Üí 0.19) / Outliers (8723 ‚Üí 1128)                                                         |\n",
    "| **indice_queima**                        | **Winsoriza√ß√£o**       |                                                                                                           |\n",
    "| **umidade_combustivel_morto_100_horas**  | **Sqrt**               | Outliers (44 ‚Üí 9)                                                                                         |\n",
    "| **umidade_combustivel_morto_1000_horas** | **Sqrt**               | Outliers (373 ‚Üí 4)                                                                                        |\n",
    "| **componente_energia_lancada**           | **Sem transforma√ß√£o**  |                                                                                                           |\n",
    "| **evapotranspiracao_real**               | **Sqrt**               | Assimetria (0.71 ‚Üí -0.00) / Outliers (3292 ‚Üí 153)                                                         |\n",
    "| **evapotranspiracao_potencial**          | **Log(x + 1)**         | Outliers (679 ‚Üí 0)                                                                                        |\n",
    "| **deficit_pressao_vapor**                | **Log(x + 1)**         | Outliers (14758 ‚Üí 672)                                                                                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYTi9RrGQd0x"
   },
   "outputs": [],
   "source": [
    "# @title Classe OutliersTreatment\n",
    "\n",
    "class OutliersTreatment(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.m_log_columns = [\n",
    "            \"precipitacao\",\n",
    "            \"velocidade_vento\",\n",
    "            \"evapotranspiracao_potencial\",\n",
    "            \"deficit_pressao_vapor\",\n",
    "        ]\n",
    "        self.m_sqrt_columns = [\n",
    "            \"umidade_especifica\",\n",
    "            \"umidade_combustivel_morto_100_horas\",\n",
    "            \"umidade_combustivel_morto_1000_horas\",\n",
    "            \"evapotranspiracao_real\",\n",
    "        ]\n",
    "        self.m_winsor_columns = [\n",
    "            \"umidade_relativa_max\",\n",
    "            \"umidade_relativa_min\",\n",
    "            \"temperatura_min\",\n",
    "            \"temperatura_max\",\n",
    "            \"indice_queima\",\n",
    "        ]\n",
    "        self.m_winsor_limits = (0.05, 0.05)\n",
    "\n",
    "    # Calcula par√¢metros necess√°rios para aplicar as transforma√ß√µes corretamente\n",
    "    def fit(self, dataframe, target=None):\n",
    "\n",
    "        # Garante que o usu√°rio esteja enviado um dataframe no formato correto\n",
    "        dataframe = dataframe if isinstance(dataframe, pd.DataFrame) else pd.DataFrame(dataframe)\n",
    "\n",
    "        # C√°lculo de offsets para garantir que n√£o haver√£o valores zerados ou negativos\n",
    "        # \"coerce\" convete valores inv√°lidos para NaN\n",
    "\n",
    "        self.m_log_offset = {}\n",
    "        for column in self.m_log_columns:\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                min_value = series.min()\n",
    "                self.m_log_offset[column] = (abs(min_value) + 1) if pd.notna(min_value) and min_value <= 0 else 1.0\n",
    "\n",
    "        self.m_sqrt_offset = {}\n",
    "        for column in self.m_sqrt_columns:\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                min_value = series.min()\n",
    "                self.m_sqrt_offset[column] = (abs(min_value) + 0.01) if pd.notna(min_value) and min_value < 0 else 0.0\n",
    "\n",
    "        # Garante que a winsoriza√ß√£o s√≥ seja feita com colunas que realmente est√£o no dataframe\n",
    "        actual_columns_to_winsor = [column for column in self.m_winsor_columns if column in dataframe.columns]\n",
    "        low_quantile, high_quantile = self.m_winsor_limits\n",
    "\n",
    "        if actual_columns_to_winsor:\n",
    "            # Converte cada coluna para num√©rico (coerces -> NaN) e calcula quantis por coluna\n",
    "            winsor_dataframe = dataframe[actual_columns_to_winsor].apply(pd.to_numeric, errors=\"coerce\")\n",
    "            self.m_low_quantile  = winsor_dataframe.quantile(low_quantile)\n",
    "            self.m_high_quantile = winsor_dataframe.quantile(1 - high_quantile)\n",
    "        else:\n",
    "            # garante atributos vazios para n√£o quebrar no transform()\n",
    "            self.m_low_quantile  = pd.Series(dtype=float)\n",
    "            self.m_high_quantile = pd.Series(dtype=float)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Aplica as transforma√ß√µes\n",
    "    def transform(self, dataframe):\n",
    "\n",
    "        # Garante que o usu√°rio esteja enviado o dataframe correto\n",
    "        dataframe = dataframe.copy() if isinstance(dataframe, pd.DataFrame) else pd.DataFrame(dataframe).copy()\n",
    "\n",
    "        # LOG\n",
    "        for column, offset in self.m_log_offset.items():\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                dataframe[column] = np.log(series + offset)\n",
    "\n",
    "        # SQRT\n",
    "        for column, offset in self.m_sqrt_offset.items():\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                dataframe[column] = np.sqrt(series + offset)\n",
    "\n",
    "        # Winsoriza√ß√£o\n",
    "        for column in self.m_low_quantile.index:\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                dataframe[column] = series.clip(lower=self.m_low_quantile[column], upper=self.m_high_quantile[column])\n",
    "\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normaliza√ß√£o dos dados\n",
    "\n",
    "S√£o utilizados para normalizar os dados num√©ricos\n",
    "- SimpleImputer -> Preenche NaN com a m√©dia da feature\n",
    "- MinMaxScaler -> Ajusta todos os valores para o intervalo entre 0 e 1\n",
    "\n",
    "No momento, n√£o est√£o sendo utilizadas features categ√≥ricas para treinar o modelo. Contudo, existe a implementa√ß√£o de uma normaliza√ß√£o provis√≥ria para as mesmas utilizando\n",
    "- SimpleImputer -> Preenche NaN com a m√©dia da feature\n",
    "- OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_HM_loh1QihZ"
   },
   "outputs": [],
   "source": [
    "# @title Classe DataNormalization\n",
    "\n",
    "'''\n",
    "categoric_cols = [season_column_name, region_column_name] \n",
    "\n",
    "def numeric_columns_selector(dataframe): \n",
    "    # Seleciona colunas num√©ricas, exceto as categ√≥ricas codificadas numericamente\n",
    "    num = dataframe.select_dtypes(include='number').columns.tolist() \n",
    "    return [column for column in num if column not in categoric_cols] \n",
    "\n",
    "categoric_columns_pipeline = Pipeline(steps=[ \n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), \n",
    "    ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False)), \n",
    "])\n",
    "'''\n",
    "\n",
    "numeric_columns_pipeline = Pipeline(steps=[ \n",
    "    ('imputer', SimpleImputer(strategy='median')), \n",
    "    ('scaler', MinMaxScaler()), \n",
    "    ]) \n",
    "\n",
    "DataNormalization = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical', numeric_columns_pipeline, make_column_selector(dtype_include='number')),\n",
    "        #('categoric', categoric_columns_pipeline, categoric_cols)\n",
    "    ],\n",
    "    remainder='passthrough'  # mant√©m quaisquer colunas n√£o listadas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pipeline de pr√©-processamento\n",
    "\n",
    "Concentra 5 passos\n",
    "1. Cria√ß√£o de features\n",
    "2. Tratamento de outliers\n",
    "3. Elimina√ß√£o de features indesejadas no treinamento dos modelos\n",
    "4. Normaliza√ß√£o dos dados\n",
    "5. Sanitiza√ß√£o dos dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que o modelo seja treinado de maneira correta, al√©m da coluna target, √© ideal que algumas outras fiquem de fora de seu escopo, s√£o elas:\n",
    "- Data\n",
    "- ID\n",
    "- Latitude\n",
    "- Longitude\n",
    "- Esta√ß√£o do ano*\n",
    "- Regi√£o*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Classe ColumnDropper\n",
    "\n",
    "# Elimina colunas n√£o desej√°veis no treinamento do modelo\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.columns = [data_column_name, id_column_name, latitude_column_name, longitude_column_name, season_column_name, region_column_name]\n",
    "    def fit(self, dataframe, target=None):\n",
    "        return self\n",
    "    def transform(self, dataframe):\n",
    "        return dataframe.drop(columns=self.columns, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun√ß√£o simples que serve para garantir que os dados est√£o no formato esperado para que ocorra o treinamento dos modelos (numpy.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Fun√ß√£o sanitizer\n",
    "\n",
    "# Remove data e fire_id, al√©m de converter o dataframe para o formato esperado pelos classificadores\n",
    "def sanitizer(features):\n",
    "    # Transforma em numpy.ndarray\n",
    "    if isinstance(features, pd.DataFrame):\n",
    "        features = features.select_dtypes(include='number')  \n",
    "    return features   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyUKC9duPwYA"
   },
   "outputs": [],
   "source": [
    "# @title Pipeline preprocess\n",
    "\n",
    "preprocess = Pipeline(steps=[\n",
    "    (\"features_creation\", FeaturesCreation()),\n",
    "    (\"outliers_treatment\", OutliersTreatment()),\n",
    "    (\"drop_unused_columns\", ColumnDropper()),\n",
    "    (\"data_normalization\", DataNormalization),\n",
    "    (\"sanitize\", FunctionTransformer(sanitizer, validate=False, feature_names_out='one-to-one'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSxExLXsXrdc"
   },
   "source": [
    "### 5. Separa√ß√£o em treino/teste\n",
    "\n",
    "<u>**Time Series Cross Validation**</u>\n",
    "\n",
    "A Time Series Cross Validation √© uma t√©cnica especializada para validar modelos quando os dados possuem ordem cronol√≥gica. Diferente das t√©cnicas tradicionais, ela respeita a estrutura temporal dos dados.\n",
    "\n",
    "Foi implemetada uma fun√ß√£o personalizada para que dados referentes ao mesmo inc√™ndio permanecessem nos mesmos grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4l1XXAgXQkt7"
   },
   "outputs": [],
   "source": [
    "# @title Fun√ß√£o group_time_series_cross_validation\n",
    "\n",
    "# Gera folds (train_idx, test_idx) respeitando ordem temporal por grupo, embargo em n√≠vel de grupo e exclus√£o m√∫tua treino/teste por grupo.\n",
    "def group_time_series_cross_validation():\n",
    "    dataframe = wildfires\n",
    "    time_column = data_column_name\n",
    "    group_column = id_column_name\n",
    "    folds_amount = 5\n",
    "    fold_groups_size = 1\n",
    "    gap_between_groups_amount = 0\n",
    "\n",
    "    # Ordena grupos pelo primeiro timestamp (data da primeira amostra do grupo)\n",
    "    first_time = (\n",
    "        dataframe[[group_column, time_column]]\n",
    "        .dropna(subset=[time_column])\n",
    "        .groupby(group_column)[time_column]\n",
    "        .min()\n",
    "        .sort_values()\n",
    "    )\n",
    "    ordered_groups = first_time.index.to_numpy()\n",
    "    ordered_groups_len = len(ordered_groups)\n",
    "\n",
    "    groups_amount_by_step = fold_groups_size\n",
    "\n",
    "    min_train_groups = max(1, fold_groups_size) # pelo menos 1\n",
    "\n",
    "    # √Çncora: √∫ltimo grupo incluso no treino\n",
    "    # Precisamos garantir espa√ßo para gap + teste √† frente\n",
    "    max_anchor = ordered_groups_len - gap_between_groups_amount - fold_groups_size\n",
    "    if max_anchor <= min_train_groups:\n",
    "        return  # n√£o h√° splits poss√≠veis\n",
    "\n",
    "    splits = 0\n",
    "    anchor = min_train_groups\n",
    "    while anchor <= max_anchor and splits < folds_amount:\n",
    "        train_groups = ordered_groups[:anchor]\n",
    "\n",
    "        test_start = anchor + gap_between_groups_amount\n",
    "        test_end = test_start + fold_groups_size\n",
    "        test_groups = ordered_groups[test_start:test_end]\n",
    "\n",
    "        train_idx = dataframe.index[dataframe[group_column].isin(train_groups)].to_numpy()\n",
    "        test_idx  = dataframe.index[dataframe[group_column].isin(test_groups)].to_numpy()\n",
    "\n",
    "        if train_idx.size and test_idx.size:\n",
    "            yield (train_idx, test_idx) # Gera√ß√£o sobre demanda (lazy evaluation)\n",
    "            splits += 1\n",
    "\n",
    "        anchor += groups_amount_by_step\n",
    "\n",
    "cross_validation = cross_validation_splits = list(group_time_series_cross_validation())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Balanceamento dos dados\n",
    "\n",
    "Ap√≥s rodar um algoritmo que contava a quantidade de amostras de cada classe e tamb√©m calculava a Raz√£o de Desbalanceamento (IR), obtivemos:\n",
    "- Classe 0 (n√£o-inc√™ndio): **2025** amostras acumuladas entre todas as folds (90%)\n",
    "- Classe 1 (inc√™ndio): **225** amostras acumuladas entre todas as folds (10%)\n",
    "- Raz√£o de desbalanceamento (IR) acumulada: **9.00x**\n",
    "\n",
    "**Observa√ß√£o**: o dataset possui mais de 300.000 linhas, mas apenas 2.500 grupos distintos (separados por inc√™ndio)\n",
    "\n",
    "O dataset apresenta uma raz√£o de desbalanceamento leve (IR < 10), o que n√£o levanta a necessidade de utiliza√ß√£o de algoritmos robustos para balanceamento.\n",
    "\n",
    "Com essa informa√ß√£o em mente, foram feitos testes em alguns modelos que aceitavam como par√¢metro o \"peso\" dos dados (class_weight = 'balanced') e os resultados foram esses:\n",
    "\n",
    "**LogisticRegression** ‚úÖ\n",
    "| M√©trica   | Antes | Depois    | Diferen√ßa |\n",
    "| --------- | ----- | --------- | --------- |\n",
    "| Accuracy  | 0.921 | **0.932** | üîº +0.011 |\n",
    "| Precision | 0.529 | **0.765** | üîº +0.236 |\n",
    "| Recall    | 0.293 | **0.787** | üîº +0.494 |\n",
    "| F1-score  | 0.335 | **0.715** | üîº +0.380 |\n",
    "| ROC AUC   | 0.975 | **0.979** | üîº +0.004 |\n",
    "\n",
    "**DecisionTreeClassifier** ‚úÖ\n",
    "| M√©trica   | Antes | Depois    | Diferen√ßa |\n",
    "| --------- | ----- | --------- | --------- |\n",
    "| Accuracy  | 0.909 | **0.940** | üîº +0.031 |\n",
    "| Precision | 0.506 | **0.826** | üîº +0.320 |\n",
    "| Recall    | 0.413 | **0.680** | üîº +0.267 |\n",
    "| F1-score  | 0.391 | **0.662** | üîº +0.271 |\n",
    "| ROC AUC   | 0.727 | **0.824** | üîº +0.097 |\n",
    "\n",
    "\n",
    "**RandomForestClassifier** ‚úÖ\n",
    "| M√©trica   | Antes | Depois    | Diferen√ßa       |\n",
    "| --------- | ----- | --------- | --------------- |\n",
    "| Accuracy  | 0.925 | **0.933** | üîº +0.008       |\n",
    "| Precision | 0.859 | **0.876** | üîº +0.017       |\n",
    "| Recall    | 0.507 | **0.560** | üîº +0.053       |\n",
    "| F1-score  | 0.498 | **0.566** | üîº +0.068       |\n",
    "| ROC AUC   | 0.947 | 0.947     | ‚ö™ sem varia√ß√£o |\n",
    "\n",
    "**XGBoost (scale_pos_weight=9.0)** üü®\n",
    "| M√©trica   | Antes     | Depois    | Diferen√ßa |\n",
    "| --------- | --------- | --------- | --------- |\n",
    "| Accuracy  | **0.935** | 0.927     | üîª -0.008 |\n",
    "| Precision | **0.830** | 0.797     | üîª -0.033 |\n",
    "| Recall    | 0.653     | **0.693** | üîº +0.040 |\n",
    "| F1-score  | 0.635     | **0.638** | üîº +0.003 |\n",
    "| ROC AUC   | **0.950** | 0.946     | üîª -0.004 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Classe BalancingCount \n",
    "\n",
    "# Classe que balancea os dados\n",
    "class BalancingCount(BaseEstimator, TransformerMixin):\n",
    "    # Vari√°veis de classe (compartilhadas entre todas as inst√¢ncias)\n",
    "    total_counts = None\n",
    "    total_calls = 0\n",
    "\n",
    "    def fit(self, dataframe, target=None):\n",
    "        if target is None:\n",
    "            raise ValueError(\"O target √© obrigat√≥rio para verificar o balanceamento.\")\n",
    "\n",
    "        # Conta as ocorr√™ncias por classe neste fold\n",
    "        counts = pd.Series(target).value_counts().sort_index()\n",
    "\n",
    "        # Atualiza contagem total global\n",
    "        if BalancingCount.total_counts is None:\n",
    "            BalancingCount.total_counts = counts.copy()\n",
    "        else:\n",
    "            # soma os valores por classe\n",
    "            BalancingCount.total_counts = BalancingCount.total_counts.add(counts, fill_value=0)\n",
    "\n",
    "        BalancingCount.total_calls += 1\n",
    "\n",
    "        # --- Exibi√ß√£o do fold atual ---\n",
    "        print(f\"\\nüìò Fold {BalancingCount.total_calls}\")\n",
    "        print(\"üìä Contagem de classes (este fold):\")\n",
    "        for classes, count in counts.items():\n",
    "            print(f\"  Classe {classes}: {count} amostras\")\n",
    "\n",
    "        ratio = counts.max() / counts.min() if len(counts) > 1 else 1.0\n",
    "        print(f\"‚öñÔ∏è  Raz√£o de desbalanceamento (este fold): {ratio:.2f}x\")\n",
    "\n",
    "        # --- Exibi√ß√£o acumulada ---\n",
    "        if BalancingCount.total_calls == 1:\n",
    "            print(\"\\nüîÑ Iniciando contagem global...\")\n",
    "        else:\n",
    "            total_ratio = BalancingCount.total_counts.max() / BalancingCount.total_counts.min()\n",
    "            print(\"\\nüìà Contagem acumulada at√© agora:\")\n",
    "            for classes, total in BalancingCount.total_counts.items():\n",
    "                print(f\"  Classe {int(classes)}: {int(total)} amostras acumuladas\")\n",
    "            print(f\"‚öñÔ∏è  Raz√£o de desbalanceamento acumulada: {total_ratio:.2f}x\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        return dataframe\n",
    "\n",
    "    @classmethod\n",
    "    def reset(cls):\n",
    "        \"\"\"Reseta contadores globais (para novo experimento).\"\"\"\n",
    "        cls.total_counts = None\n",
    "        cls.total_calls = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Treinamento dos modelos\n",
    "\n",
    "Os modelos treinados foram\n",
    "- Dummy (mais frequente)\n",
    "- Regress√£o Log√≠stica\n",
    "- √Årvore de Decis√£o\n",
    "- Random Forest\n",
    "- Naive Bayes\n",
    "- KNN\n",
    "- Gradient Boosting **(NOVO)**\n",
    "- AdaBoost **(NOVO)**\n",
    "- HistGradientBoosting **(NOVO)**\n",
    "- RidgeClassifier **(NOVO)**\n",
    "- XGBoost **(NOVO)**\n",
    "- CatBoost **(NOVO)**\n",
    "- LightGBM **(NOVO - com problemas)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**M√©tricas para avalia√ß√£o dos modelos**</u>\n",
    "\n",
    "| **M√©trica**                | **Descri√ß√£o**                                                                               | **Interpreta√ß√£o ideal**                                                                  |\n",
    "| -------------------------- | ------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n",
    "| **Accuracy**               | Propor√ß√£o de acertos totais (tanto positivos quanto negativos).                             | Boa para classes balanceadas, mas pode mascarar desempenho ruim em classes minorit√°rias. |\n",
    "| **Precision**              | Entre as previs√µes positivas, quantas realmente eram positivas.                             | Alta precis√£o = poucos falsos positivos.                                                 |\n",
    "| **Recall (Sensibilidade)** | Entre os casos realmente positivos, quantos foram identificados corretamente.               | Alto recall = poucos falsos negativos.                                                   |\n",
    "| **F1-score**               | M√©dia harm√¥nica entre precis√£o e recall, equilibrando ambos.                                | Ideal quando h√° desbalanceamento de classes.                                             |\n",
    "| **ROC AUC**                | Mede a capacidade global do modelo em separar as classes (0.5 = aleat√≥rio; 1.0 = perfeito). | Pr√≥ximo de 1 indica boa separabilidade.                                                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Escolha dos modelos e das m√©tricas\n",
    "\n",
    "modelos = {\n",
    "    \"Dummy (mais frequente)\": DummyClassifier(strategy=\"most_frequent\"), # Baseline simples\n",
    "\n",
    "    # Sem balanceamento\n",
    "    # \"Regress√£o Log√≠stica\": LogisticRegression(C=0.5, penalty='l2', solver='liblinear', max_iter=2000), # Regulariza√ß√£o L2 leve\n",
    "    # \"√Årvore de Decis√£o\": DecisionTreeClassifier(max_depth=8, min_samples_split=4, min_samples_leaf=2, random_state=42), # Controle de profundidade e tamanho da folha\n",
    "    # \"Random Forest\": RandomForestClassifier(n_estimators=150, max_depth=10, min_samples_split=5,random_state=42, n_jobs=-1), # Mais √°rvores e profundidade moderada\n",
    "\n",
    "    # Com balanceamento\n",
    "    \"Regress√£o Log√≠stica\": LogisticRegression(C=0.5, penalty='l2', solver='liblinear', max_iter=2000, class_weight = 'balanced'), # Regulariza√ß√£o L2 leve\n",
    "    \"√Årvore de Decis√£o\": DecisionTreeClassifier(max_depth=8, min_samples_split=4, min_samples_leaf=2, random_state=42, class_weight = 'balanced'), # Controle de profundidade e tamanho da folha\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=150, max_depth=10, min_samples_split=5,random_state=42, n_jobs=-1, class_weight = 'balanced'), # Mais √°rvores e profundidade moderada\n",
    "   \n",
    "    \"Naive Bayes\": GaussianNB(var_smoothing=1e-8), # Suaviza√ß√£o leve (mais est√°vel)\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=7, weights='distance'), # Mais vizinhos e dist√¢ncia ponderada\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42), # Par√¢metros leves conforme aula\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=42), # Taxa de aprendizado menor\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(max_iter=150, learning_rate=0.05, max_depth=5, random_state=42), # Mais itera√ß√µes e taxa menor\n",
    "    \"RidgeClassifier\": RidgeClassifier(alpha=1.0), # Regulariza√ß√£o leve\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', n_estimators=150, learning_rate=0.05, max_depth=5, subsample=0.8, colsample_bytree=0.8, random_state=42), #  Par√¢metros t√≠picos de equil√≠brio (aula 15 parte 4)\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=150, learning_rate=0.05, depth=5,verbose=0, random_state=42), # Taxa de aprendizado reduzida e itera√ß√µes extras\n",
    "    # \"LightGBM\": LGBMClassifier(n_estimators=150, learning_rate=0.05, num_leaves=31, max_depth=6, random_state=42) # Profundidade controlada e taxa moderada\n",
    "}\n",
    "\n",
    "# M√©tricas de avalia√ß√£o\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': make_scorer(recall_score, zero_division=0),\n",
    "    'f1': make_scorer(f1_score, zero_division=0),\n",
    "    'roc_auc': 'roc_auc' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Algoritmo de treinamento\n",
    "\n",
    "features = wildfires.drop(columns=target_column_name)\n",
    "target = wildfires[target_column_name].astype(int)\n",
    "\n",
    "periodos_testes = [30, 60, 90, 120, 150, 180, 210]\n",
    "\n",
    "resultados_gerais = []\n",
    "\n",
    "for periodo in periodos_testes:\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    window_days = periodo\n",
    "    print(\"Per√≠odo de dias: \", periodo)\n",
    "\n",
    "    for nome, modelo in modelos.items():\n",
    "        # print(f\"Treinando modelo: {nome}...\")\n",
    "        # BalancingCount.reset()\n",
    "\n",
    "        try:\n",
    "            pipeline = Pipeline([\n",
    "                (\"preprocess\", preprocess),\n",
    "                (\"nan_shield\", SimpleImputer(strategy=\"constant\", fill_value=0.0)), # Blindagem contra NaN\n",
    "                # (\"balancing\", BalancingCount()),\n",
    "                (\"classificator\", modelo)\n",
    "            ])\n",
    "\n",
    "            scores = cross_validate(\n",
    "                pipeline,\n",
    "                features,\n",
    "                target,\n",
    "                cv=cross_validation,\n",
    "                scoring=scoring,\n",
    "                n_jobs=1\n",
    "            )\n",
    "\n",
    "            resultados.append({\n",
    "                \"Modelo\": nome,\n",
    "                \"Accuracy\": np.mean(scores['test_accuracy']),\n",
    "                \"Precision\": np.mean(scores['test_precision']),\n",
    "                \"Recall\": np.mean(scores['test_recall']),\n",
    "                \"F1-score\": np.mean(scores['test_f1']),\n",
    "                \"ROC AUC\": np.mean(scores['test_roc_auc']),\n",
    "            })\n",
    "\n",
    "            resultados_gerais.append({\n",
    "                \"Modelo\": nome,\n",
    "                \"F1-score\": np.mean(scores['test_f1']),\n",
    "                \"Tempo\": periodo\n",
    "            })\n",
    "\n",
    "            # print(f\" Modelo {nome} treinado com sucesso.\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            # print(f\" Erro ao rodar o modelo {nome}: {e}\\n\")\n",
    "\n",
    "    # Mostra os resultados por per√≠odo\n",
    "    df_resultados = pd.DataFrame(resultados).sort_values(\"F1-score\", ascending=False)\n",
    "    display(df_resultados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Compara√ß√£o de resultados\n",
    "\n",
    "resultados_por_modelo = {}\n",
    "\n",
    "# Agrupa os resultados por modelo\n",
    "for entrada in resultados_gerais:\n",
    "    nome_modelo = entrada[\"Modelo\"]\n",
    "    if nome_modelo not in resultados_por_modelo:\n",
    "        resultados_por_modelo[nome_modelo] = []\n",
    "    resultados_por_modelo[nome_modelo].append(entrada)\n",
    "\n",
    "# Ordena os resultados de cada modelo\n",
    "for nome_modelo, lista_resultados in resultados_por_modelo.items():\n",
    "    lista_ordenada = sorted(\n",
    "        lista_resultados,\n",
    "        key=lambda x: x[\"F1-score\"],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n==== Modelo: {nome_modelo} ====\")\n",
    "    for posicao, item in enumerate(lista_ordenada, start=1):\n",
    "        print(f\"{posicao}¬∫ lugar | Tempo: {item['Tempo']:>3} dias | F1-score: {item['F1-score']:.4f}\")\n",
    "\n",
    "# Ordena os resultados de maneira gerais\n",
    "resultados_gerais_ordenados = sorted(\n",
    "    resultados_gerais,\n",
    "    key=lambda x: x[\"F1-score\"],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(\"\\n==== Ranking geral (todos os modelos e per√≠odos) ====\")\n",
    "for posicao, item in enumerate(resultados_gerais_ordenados, start=1):\n",
    "    print(f\"{posicao}¬∫ lugar | Modelo: {item['Modelo']} | Tempo: {item['Tempo']:>3} dias | F1-score: {item['F1-score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
