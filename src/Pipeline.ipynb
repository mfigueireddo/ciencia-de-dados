{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30HRRwSCAdKV"
   },
   "source": [
    "## Etapas\n",
    "\n",
    "1. Criação de features\n",
    "2. Tratamento de outliers\n",
    "3. Normalização dos dados\n",
    "4. Ajustes nos dados\n",
    "5. Pipeline\n",
    "6. Separação em treino/teste\n",
    "7. Treinando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4F661DIPyJH"
   },
   "outputs": [],
   "source": [
    "# @title Importação de bibliotecas\n",
    "\n",
    "# Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Criação de features\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Tratamento de outliers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Ajustes nos dados\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Treinamento do modelo \n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 20320,
     "status": "error",
     "timestamp": 1762447144734,
     "user": {
      "displayName": "Pedro",
      "userId": "13797227125436573976"
     },
     "user_tz": 180
    },
    "id": "x-cs77oXAWpf",
    "outputId": "1e7a32a7-0679-4049-a587-db4439c1244b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive not mounted, so nothing to flush and unmount.\n",
      "Mounted at /content/drive\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/QuickAccess/pre-pipeline_wildfires.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3361038527.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Leitura do arquivo .csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mwildfires\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/QuickAccess/pre-pipeline_wildfires.csv'"
     ]
    }
   ],
   "source": [
    "# @title Carregamento do dataset\n",
    "\n",
    "github_link = \"https://github.com/mfigueireddo/ciencia-de-dados/blob/ba579573c5b8a9246ca04f7da29bc2c74c8b362c/datasets/pre-pipeline_wildfires.parquet\"\n",
    "url = github_link.replace(\"/blob/\", \"/raw/\")\n",
    "\n",
    "local_file_path = Path(\"/content/raw_wildfires.parquet\")\n",
    "\n",
    "# Faz uma requisição HTTP GET ao GitHub\n",
    "with requests.get(url, stream=True) as request:\n",
    "\n",
    "    request.raise_for_status() # Confere se houve êxito\n",
    "\n",
    "    with open(local_file_path , \"wb\") as file:\n",
    "        for chunk in request.iter_content(chunk_size=1024*1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "\n",
    "wildfires = pd.read_parquet(local_file_path,  engine=\"pyarrow\") # Leitura realizada com a engine pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Criação de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features geradas\n",
    "- Estação do ano baseada no hemisfério Sul (0=Verão, 1=Outono, 2=Inverno, 3=Primavera)\n",
    "- Região do incêndio\n",
    "- Soma da precipitação nos últimos 14 dias\n",
    "- Média de temperatura máxima nos últimos 7 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUCqwXwyFR2O"
   },
   "outputs": [],
   "source": [
    "# @title Classe responsável pela criação de features\n",
    "\n",
    "class FeaturesCreation(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 # Colunas utilizadas\n",
    "                 date_column='data', group_column='fire_id', \n",
    "                 latitude_column='latitude', longitude_column='longitude', \n",
    "                 precipitation_column='precipitacao', max_temperature_column='temp_max',\n",
    "\n",
    "                 # Parâmetros personalizados para criação das features\n",
    "                 precipitation_window_days=14, max_temperature_window_days=7,\n",
    "\n",
    "                 # Parâmetros do DBSCAN\n",
    "                 max_radius_km=1.0, min_samples=5):\n",
    "        \n",
    "        self.date_column = date_column\n",
    "        self.group_column = group_column\n",
    "        self.latitude_column = latitude_column\n",
    "        self.longitude_column = longitude_column\n",
    "        self.precipitation_column = precipitation_column\n",
    "        self.max_temperature_column = max_temperature_column\n",
    "        self.precipitation_window_days = precipitation_window_days\n",
    "        self.max_temperature_window_days = max_temperature_window_days\n",
    "        self.max_radius_km = max_radius_km\n",
    "        self.min_samples = min_samples\n",
    "\n",
    "        # Objetos aprendidos no fit\n",
    "        self._dbscan_ = None\n",
    "        self._nearest_neighbors_core_ = None\n",
    "        self._core_labels_ = None\n",
    "        self._max_radius_ = None\n",
    "\n",
    "    # Conversão necessária para o DBSCAN\n",
    "    @staticmethod\n",
    "    def _to_radians(latitude_or_longitude):\n",
    "        return np.radians(latitude_or_longitude.astype(float))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _month_to_season_south(month):\n",
    "        # Estações (aprox.) para hemisfério sul:\n",
    "        # Verão:     Dez(12)-Fev(2) -> 0\n",
    "        # Outono:    Mar(3)-Mai(5)  -> 1\n",
    "        # Inverno:   Jun(6)-Ago(8)  -> 2\n",
    "        # Primavera: Set(9)-Nov(11) -> 3\n",
    "        if month in (12, 1, 2):   return 0\n",
    "        if month in (3, 4, 5):    return 1\n",
    "        if month in (6, 7, 8):    return 2\n",
    "        return 3  # 9,10,11\n",
    "\n",
    "    def fit(self, dataframe, y=None):\n",
    "\n",
    "        dataframe = dataframe.copy()\n",
    "\n",
    "        # Desativa o DBSCAN caso não haja latitude e longitude\n",
    "        missing_cols = [c for c in [self.latitude_column, self.longitude_column] if c not in dataframe.columns]\n",
    "        if missing_cols:\n",
    "            self._dbscan_ = None\n",
    "            self._nearest_neighbors_core_ = None\n",
    "            self._core_labels_ = None\n",
    "            self._max_radius_ = None\n",
    "            return self\n",
    "\n",
    "        # Latitude/Longitude -> Radianos\n",
    "        latitude_or_longitude = dataframe[[self.latitude_column, self.longitude_column]].to_numpy()\n",
    "        latitude_or_longitude_radians = self._to_radians(latitude_or_longitude)\n",
    "\n",
    "        # Raio/EPS (km -> radianos)\n",
    "        earth_radius = 6371.0\n",
    "        self._max_radius_ = self.max_radius_km / earth_radius\n",
    "\n",
    "        # Executa o DBSCAN\n",
    "        dbscan = DBSCAN(eps=self._max_radius_, min_samples=self.min_samples, metric='haversine')\n",
    "        dbscan.fit(latitude_or_longitude_radians)\n",
    "        self._dbscan_ = dbscan\n",
    "\n",
    "        # Treina um NearestNeighbors apenas nos pontos-core para atribuição de novos pontos à clusters existentes em transform()\n",
    "        core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "        if hasattr(dbscan, 'core_sample_indices_') and len(dbscan.core_sample_indices_) > 0:\n",
    "            core_mask[dbscan.core_sample_indices_] = True\n",
    "            core_points = latitude_or_longitude_radians[core_mask]\n",
    "            core_labels = dbscan.labels_[core_mask]\n",
    "\n",
    "            if len(core_points) > 0:\n",
    "                nearest_neighbors = NearestNeighbors(n_neighbors=1, metric='haversine')\n",
    "                nearest_neighbors.fit(core_points)\n",
    "                self._nearest_neighbors_core_ = nearest_neighbors\n",
    "                self._core_labels_ = core_labels\n",
    "            else:\n",
    "                self._nearest_neighbors_core_ = None\n",
    "                self._core_labels_ = None\n",
    "        else:\n",
    "            self._nearest_neighbors_core_ = None\n",
    "            self._core_labels_ = None\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Rotula novos pontos\n",
    "    def _assign_dbscan_labels(self, dataframe):\n",
    "\n",
    "        # Se não tivemos lat/lon ou DBSCAN treinado, devolve NaN\n",
    "        if self._nearest_neighbors_core_ is None or self._core_labels_ is None or self._max_radius_ is None:\n",
    "            return pd.Series([-1] * len(dataframe), index=dataframe.index, dtype='int64')\n",
    "\n",
    "        # Latitude/Longitude -> radiano\n",
    "        latitude_or_longitude = dataframe[[self.latitude_column, self.longitude_column]].to_numpy()\n",
    "        latitude_or_longitude_radians = self._to_radians(latitude_or_longitude)\n",
    "\n",
    "        # Atribui rótulo do core mais próximo, desde que dentro do Raio/EPS\n",
    "        distances, indices = self._nearest_neighbors_core_.kneighbors(latitude_or_longitude_radians, n_neighbors=1, return_distance=True)\n",
    "        distances = distances.reshape(-1)\n",
    "        indices = indices.reshape(-1)\n",
    "\n",
    "        labels = np.full(len(dataframe), -1, dtype='int64')\n",
    "        within = distances <= self._max_radius_\n",
    "        labels[within] = self._core_labels_[indices[within]]\n",
    "\n",
    "        return pd.Series(labels, index=dataframe.index, dtype='int64')\n",
    "\n",
    "    # Adiciona médias móveis e soma pro grupo de incêndio\n",
    "    def _add_temporal_rollings(self, dataframe):\n",
    "        \n",
    "        # Ordena por grupo e tempo para garantir rolling correto\n",
    "        if self.group_column in dataframe.columns and self.date_column in dataframe.columns:\n",
    "            dataframe = dataframe.sort_values([self.group_column, self.date_column])\n",
    "        else:\n",
    "            # Se faltar algo, só ordena por data se existir\n",
    "            if self.date_column in dataframe.columns:\n",
    "                dataframe = dataframe.sort_values(self.date_column)\n",
    "\n",
    "        # Rolling de precipitação (soma 14d)\n",
    "        if self.precipitation_column in dataframe.columns:\n",
    "            dataframe['soma_precipitacao_14d'] = (\n",
    "                dataframe.groupby(self.group_column, dropna=False)[self.precipitation_column]\n",
    "                  .rolling(self.precipitation_window_days, min_periods=1)\n",
    "                  .sum()\n",
    "                  .reset_index(level=0, drop=True)\n",
    "            )\n",
    "        else:\n",
    "            dataframe['soma_precipitacao_14d'] = np.nan\n",
    "\n",
    "        # Rolling de temp máxima (média 7d)\n",
    "        if self.max_temperature_column in dataframe.columns:\n",
    "            dataframe['media_temp_max_7d'] = (\n",
    "                dataframe.groupby(self.group_column, dropna=False)[self.max_temperature_column]\n",
    "                  .rolling(self.max_temperature_window_days, min_periods=1)\n",
    "                  .mean()\n",
    "                  .reset_index(level=0, drop=True)\n",
    "            )\n",
    "        else:\n",
    "            dataframe['media_temp_max_7d'] = np.nan\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    # Aplica transformações e cria as novas features\n",
    "    def transform(self, dataframe, target=None):\n",
    "        \n",
    "        # Trabalha em DataFrame para manter nomes/índices\n",
    "        dataframe = pd.DataFrame(dataframe).copy()\n",
    "\n",
    "        # 1) Estação do ano (numérica, 0..3)\n",
    "        if self.date_column in dataframe.columns:\n",
    "            # Garante dtype datetime\n",
    "            dataframe[self.date_column] = pd.to_datetime(dataframe[self.date_column], errors='coerce')\n",
    "            estacao = dataframe[self.date_column].dt.month.map(self._month_to_season_south).astype('Int64')\n",
    "            dataframe['estacao_ano_id'] = estacao.astype('float').astype('Int64')  # evita problemas de NaN -> imputar depois\n",
    "            dataframe['estacao_ano_id'] = dataframe['estacao_ano_id'].astype('float')\n",
    "        else:\n",
    "            dataframe['estacao_ano_id'] = np.nan\n",
    "\n",
    "        # 2) Região por DBSCAN (atribuída por NN para dados novos)\n",
    "        if all(c in dataframe.columns for c in [self.latitude_column, self.longitude_column]):\n",
    "            dataframe['regiao_incendio'] = self._assign_dbscan_labels(dataframe).astype('int64')\n",
    "        else:\n",
    "            dataframe['regiao_incendio'] = -1\n",
    "\n",
    "        # 3) Rollings temporais por grupo\n",
    "        dataframe = self._add_temporal_rollings(dataframe)\n",
    "\n",
    "        return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustes nos dados\n",
    "\n",
    "As features estação do ano e região do incêndio que foram criadas precisam ser tratadas. \n",
    "Isso porque seus valores são categóricos e não há relação numérica entre eles. \n",
    "Isto será feito na normalização dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXWH4spcYhkY"
   },
   "source": [
    "## 2. Tratamento de outliers\n",
    "\n",
    "**Transformação Logarítmica**\n",
    "- Aplica log(x) ou log(x+constante) para valores positivos\n",
    "- Muito eficaz para dados com distribuição assimétrica positiva\n",
    "- Comprime valores grandes e expande valores pequenos\n",
    "- Fórmula: X_log = log(X + c), onde c evita log(0)\n",
    "\n",
    "**Transformação Raiz Quadrada**\n",
    "- Menos drástica que a transformação logarítmica\n",
    "- Útil para dados de contagem e variáveis positivamente assimétricas\n",
    "- Fórmula: X_sqrt = sqrt(X)\n",
    "\n",
    "**Winsorização (Capping/Clipping)**\n",
    "\n",
    "A **Winsorização** é uma técnica de tratamento de outliers que **limita valores extremos** sem removê-los completamente. Em vez de excluir outliers, substituímos os valores extremos pelos valores de percentis específicos.\n",
    "\n",
    "**Como funciona:**\n",
    "- Define-se limites baseados em percentis (ex: 5º e 95º percentil)\n",
    "- Valores abaixo do limite inferior são substituídos pelo valor do limite inferior\n",
    "- Valores acima do limite superior são substituídos pelo valor do limite superior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmhrgtvXBUL8"
   },
   "source": [
    "| Variável                                 | Melhor método         | Justificativa (1 linha)                                                                                   |\n",
    "| :--------------------------------------- | :-------------------- | :-------------------------------------------------------------------------------------------------------- |\n",
    "| **precipitacao**                         | **Log(x + 1)**        | Reduziu fortemente a assimetria (7.87 → 2.59) e manteve os limites IQR estáveis — ideal para cauda longa. |\n",
    "| **umidade_relativa_max**                 | **Winsorizar 5%-5%**  | Cortou outliers (23 → 0) e melhorou levemente a simetria; log/sqrt inflaram valores.                      |\n",
    "| **umidade_relativa_min**                 | **Winsorizar 5%-5%**  | Assimetria e outliers foram totalmente corrigidos (1155 → 0).                                             |\n",
    "| **umidade_especifica**                   | **Sqrt**              | Melhor simetria (0.89 → 0.16) e forte redução de outliers (6967 → 2102).                                  |\n",
    "| **radiacao_solar**                       | **Sem transformação** | Já simétrica e sem outliers; log piorou, winsor apenas repete.                                            |\n",
    "| **temperatura_min**                      | **Winsorizar 5%-5%**  | Remoção completa de outliers (5066 → 0) e leve ganho de simetria.                                         |\n",
    "| **temperatura_max**                      | **Winsorizar 5%-5%**  | Mesmo comportamento de `temperatura_min`.                                                                 |\n",
    "| **velocidade_vento**                     | **Log(x + 1)**        | Reduziu assimetria (1.23 → 0.19) e outliers (8723 → 1128) sem eliminar extremos reais.                    |\n",
    "| **indice_queima**                        | **Winsorizar 5%-5%**  | Log e sqrt aumentaram outliers via IQR; winsor eliminou-os com mínima distorção.                          |\n",
    "| **umidade_combustivel_morto_100_horas**  | **Sqrt**              | Forte queda de outliers (44 → 9) e leve suavização de forma.                                              |\n",
    "| **umidade_combustivel_morto_1000_horas** | **Sqrt**              | Reduziu outliers (373 → 4) mantendo distribuição coerente.                                                |\n",
    "| **componente_energia_lancada**           | **Sem transformação** | Já equilibrada; transformações criam falsos outliers.                                                     |\n",
    "| **evapotranspiracao_real**               | **Sqrt**              | Melhorou drasticamente a simetria (0.71 → -0.00) e reduziu outliers (3292 → 153).                         |\n",
    "| **evapotranspiracao_potencial**          | **Log(x + 1)**        | Skew caiu (0.53 → -0.36) e outliers zeraram (679 → 0).                                                    |\n",
    "| **deficit_pressao_vapor**                | **Log(x + 1)**        | Alta cauda direita suavizada (1.46 → 0.52) e outliers despencaram (14758 → 672).                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYTi9RrGQd0x"
   },
   "outputs": [],
   "source": [
    "# @title Classe responsável pelo tratamento dos outliers\n",
    "\n",
    "class OutliersTreatment(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, \n",
    "                 # Colunas a serem aplicadas as transformações\n",
    "                 log_columns=None, sqrt_columns=None, winsor_columns=None, \n",
    "\n",
    "                 # Frações dos dados a serem cortadas em cada extremidade\n",
    "                 winsor_limits=(0.05, 0.05)):\n",
    "        \n",
    "        self.log_columns = log_columns or []\n",
    "        self.sqrt_columns = sqrt_columns or []\n",
    "        self.winsor_columns = winsor_columns or []\n",
    "        self.winsor_limits = winsor_limits\n",
    "\n",
    "    # Calcula parâmetros necessários para aplicar as transformações corretamente\n",
    "    def fit(self, dataframe, target=None):\n",
    "\n",
    "        # Garante que o usuário esteja enviado um dataframe no formato correto\n",
    "        dataframe = dataframe if isinstance(dataframe, pd.DataFrame) else pd.DataFrame(dataframe)\n",
    "\n",
    "        # Cálculo de offsets para garantir que não haverão valores zerados ou negativos\n",
    "        # \"coerce\" convete valores inválidos para NaN\n",
    "\n",
    "        self.log_offset_ = {}\n",
    "        for column in self.log_columns:\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                min_value = series.min()\n",
    "                self.log_offset_[column] = (abs(min_value) + 1) if pd.notna(min_value) and min_value <= 0 else 1.0\n",
    "\n",
    "        self.sqrt_offset_ = {}\n",
    "        for column in self.sqrt_columns:\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                min_value = series.min()\n",
    "                self.sqrt_offset_[column] = (abs(min_value) + 0.01) if pd.notna(min_value) and min_value < 0 else 0.0\n",
    "\n",
    "        # Garante que a winsorização só seja feita com colunas que realmente estão no dataframe\n",
    "        actual_columns_to_winsor = [column for column in self.winsor_columns if column in dataframe.columns]\n",
    "        low_quantile, high_quantile = self.winsor_limits\n",
    "\n",
    "        if actual_columns_to_winsor:\n",
    "            # Converte cada coluna para numérico (coerces -> NaN) e calcula quantis por coluna\n",
    "            df_w = dataframe[actual_columns_to_winsor].apply(pd.to_numeric, errors=\"coerce\")\n",
    "            self.low_  = df_w.quantile(low_quantile)\n",
    "            self.high_ = df_w.quantile(1 - high_quantile)\n",
    "        else:\n",
    "            # garante atributos vazios para não quebrar no transform()\n",
    "            self.low_  = pd.Series(dtype=float)\n",
    "            self.high_ = pd.Series(dtype=float)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Aplica as transformações\n",
    "    def transform(self, dataframe):\n",
    "\n",
    "        # Garante que o usuário esteja enviado o dataframe correto\n",
    "        dataframe = dataframe.copy() if isinstance(dataframe, pd.DataFrame) else pd.DataFrame(dataframe).copy()\n",
    "\n",
    "        # LOG\n",
    "        for column, offset in self.log_offset_.items():\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                dataframe[column] = np.log(series + offset)\n",
    "\n",
    "        # SQRT\n",
    "        for column, offset in self.sqrt_offset_.items():\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                dataframe[column] = np.sqrt(series + offset)\n",
    "\n",
    "        # Winsorização\n",
    "        for column in self.low_.index:\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                dataframe[column] = series.clip(lower=self.low_[column], upper=self.high_[column])\n",
    "\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalização dos dados\n",
    "\n",
    "TO-DO: trazer o algoritmo desenvolvido para normalização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_HM_loh1QihZ"
   },
   "outputs": [],
   "source": [
    "# @title Algoritmo de normalização\n",
    "\n",
    "# Colunas categóricas criadas no passo de features \n",
    "_categoric_cols = ['estacao_ano_num', 'regiao_incendio'] \n",
    "\n",
    "def _numeric_columns_selector(dataframe): \n",
    "    # Seleciona colunas numéricas, exceto as categóricas codificadas numericamente\n",
    "    num = dataframe.select_dtypes(include='number').columns.tolist() \n",
    "    return [column for column in num if column not in _categoric_cols] \n",
    "\n",
    "_numeric_columns_pipe = Pipeline(steps=[ \n",
    "    ('imputer', SimpleImputer(strategy='median')), \n",
    "    ('scaler', StandardScaler()), \n",
    "    ]) \n",
    "\n",
    "_categoric_columns_pipe = Pipeline(steps=[ \n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), \n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False)), \n",
    "])\n",
    "\n",
    "DataNormalization = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', _numeric_columns_pipe, _numeric_columns_selector),\n",
    "        ('cat', _categoric_columns_pipe, _categoric_cols)\n",
    "    ],\n",
    "    remainder='passthrough'  # mantém quaisquer colunas não listadas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyUKC9duPwYA"
   },
   "outputs": [],
   "source": [
    "# @title Código da Pipeline\n",
    "\n",
    "log_columns = [\n",
    "    \"precipitacao\",\n",
    "    \"velocidade_vento\",\n",
    "    \"evapotranspiracao_potencial\",\n",
    "    \"deficit_pressao_vapor\",\n",
    "]\n",
    "sqrt_columns = [\n",
    "    \"umidade_especifica\",\n",
    "    \"umidade_combustivel_morto_100_horas\",\n",
    "    \"umidade_combustivel_morto_1000_horas\",\n",
    "    \"evapotranspiracao_real\",\n",
    "]\n",
    "winsor_columns = [\n",
    "    \"umidade_relativa_max\",\n",
    "    \"umidade_relativa_min\",\n",
    "    \"temperatura_min\",\n",
    "    \"temperatura_max\",\n",
    "    \"indice_queima\",\n",
    "]\n",
    "\n",
    "preprocess = Pipeline(steps=[\n",
    "    (\"features_creation\", FeaturesCreation()),\n",
    "    (\"outliers_treatment\", OutliersTreatment(\n",
    "        log_cols=log_columns,\n",
    "        sqrt_cols=sqrt_columns,\n",
    "        winsor_cols=winsor_columns\n",
    "    )),\n",
    "    (\"data_normalization\", DataNormalization)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSxExLXsXrdc"
   },
   "source": [
    "## 6. Separação em treino/teste\n",
    "\n",
    "### Time Series Cross Validation\n",
    "\n",
    "A Time Series Cross Validation é uma técnica especializada para validar modelos quando os dados possuem ordem cronológica. Diferente das técnicas tradicionais, ela respeita a estrutura temporal dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4l1XXAgXQkt7"
   },
   "outputs": [],
   "source": [
    "# @title Algoritmo de separação\n",
    "\n",
    "# Gera folds (train_idx, test_idx) respeitando ordem temporal por grupo, embargo em nível de grupo e exclusão mútua treino/teste por grupo.\n",
    "def group_time_series_cross_validation(\n",
    "    dataframe: pd.DataFrame,\n",
    "    time_column: str,\n",
    "    group_column: str,\n",
    "    *,\n",
    "    folds_amount: int = 5,\n",
    "    fold_groups_size: int = 1,\n",
    "    gap_between_groups_amount: int = 0,\n",
    "    is_train_expanding_by_each_split: bool = True,\n",
    "    min_train_groups: int | None = None,\n",
    "    groups_amount_by_step: int | None = None,\n",
    "):\n",
    "    # Ordena grupos pelo primeiro timestamp\n",
    "    first_time = (\n",
    "        dataframe[[group_column, time_column]]\n",
    "        .dropna(subset=[time_column])\n",
    "        .groupby(group_column)[time_column]\n",
    "        .min()\n",
    "        .sort_values()\n",
    "    )\n",
    "    ordered_groups = first_time.index.to_numpy()\n",
    "    ordered_groups_len = len(ordered_groups)\n",
    "\n",
    "    if groups_amount_by_step is None:\n",
    "        groups_amount_by_step = fold_groups_size\n",
    "\n",
    "    if min_train_groups is None:\n",
    "        min_train_groups = max(1, fold_groups_size) # pelo menos 1\n",
    "\n",
    "    # Âncora: último grupo incluso no treino\n",
    "    # Precisamos garantir espaço para gap + teste à frente\n",
    "    max_anchor = ordered_groups_len - gap_between_groups_amount - fold_groups_size\n",
    "    if max_anchor <= min_train_groups:\n",
    "        return  # não há splits possíveis\n",
    "\n",
    "    splits = 0\n",
    "    anchor = min_train_groups\n",
    "    while anchor <= max_anchor and splits < folds_amount:\n",
    "        if is_train_expanding_by_each_split:\n",
    "            train_groups = ordered_groups[:anchor]\n",
    "        else:\n",
    "            start = max(0, anchor - min_train_groups)\n",
    "            train_groups = ordered_groups[start:anchor]\n",
    "\n",
    "        test_start = anchor + gap_between_groups_amount\n",
    "        test_end = test_start + fold_groups_size\n",
    "        test_groups = ordered_groups[test_start:test_end]\n",
    "\n",
    "        train_idx = dataframe.index[dataframe[group_column].isin(train_groups)].to_numpy()\n",
    "        test_idx  = dataframe.index[dataframe[group_column].isin(test_groups)].to_numpy()\n",
    "\n",
    "        if train_idx.size and test_idx.size:\n",
    "            yield (train_idx, test_idx)\n",
    "            splits += 1\n",
    "\n",
    "        anchor += groups_amount_by_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Executando separação\n",
    "\n",
    "cross_validation_splits = list(group_time_series_cross_validation(\n",
    "    df=wildfires,\n",
    "    time_col='data',\n",
    "    group_col='fire_id',\n",
    "    n_splits=5,\n",
    "    test_groups_size=1,\n",
    "    gap_groups=0,\n",
    "    expanding=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Treinando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Algoritmo de treinamento\n",
    "\n",
    "# Separar features e target (mantém 'data' e 'fire_id' porque o preprocess usa essas colunas)\n",
    "features = wildfires.drop(columns=[\"houve_incendio\"])\n",
    "target = wildfires[\"houve_incendio\"].astype(int)\n",
    "\n",
    "resultados = []\n",
    "\n",
    "modelos = {\n",
    "    \"Dummy (mais frequente)\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"Regressão Logística\": LogisticRegression(max_iter=1000),\n",
    "    \"Árvore de Decisão\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=50, n_jobs=-1),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"SVM (linear)\": SVC(kernel='linear')  # Deixe por último, ele n ta indo (apagar quando for rodar, depois vou tentar arrumar)\n",
    "}\n",
    "\n",
    "# Usar os folds temporais por grupo já definidos anteriormente\n",
    "# (cv_splits veio da célula \"Pipeline\" com group_time_series_cv(...))\n",
    "cross_validation = cross_validation_splits\n",
    "\n",
    "# Métricas de avaliação\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': make_scorer(recall_score, zero_division=0),\n",
    "    'f1': make_scorer(f1_score, zero_division=0),\n",
    "    'roc_auc': 'roc_auc'  # deixa o sklearn decidir entre predict_proba/decision_function\n",
    "}\n",
    "\n",
    "# Remove data e fire_id, além de converte o dataframe para o formato esperado pelos classificadores\n",
    "def _sanitize_after_preprocess(features):\n",
    "    # Transforma em numpy.ndarray\n",
    "    if isinstance(features, pd.DataFrame):\n",
    "        cols = [column for column in features.columns if column not in ('data', 'fire_id')]\n",
    "        features = features[cols]\n",
    "        features = features.select_dtypes(include='number')\n",
    "    return features\n",
    "\n",
    "_sanitize = FunctionTransformer(_sanitize_after_preprocess, validate=False, feature_names_out='one-to-one')\n",
    "\n",
    "for nome, modelo in modelos.items():\n",
    "    print(f\"Treinando modelo: {nome}...\")\n",
    "    try:\n",
    "        pipeline = Pipeline([\n",
    "            (\"preprocess\", preprocess), # Executa a pipeline de pré-processamento\n",
    "            (\"sanitize\", _sanitize), # Formata o dataframe\n",
    "            (\"final_imputer\", SimpleImputer(strategy=\"constant\", fill_value=0.0)), # Blindagem contra NaN\n",
    "            (\"clf\", modelo)\n",
    "        ])\n",
    "\n",
    "        scores = cross_validate(pipeline, features, target, cv=cross_validation, scoring=scoring)\n",
    "\n",
    "        resultados.append({\n",
    "            \"Modelo\": nome,\n",
    "            \"Accuracy\": np.mean(scores['test_accuracy']),\n",
    "            \"Precision\": np.mean(scores['test_precision']),\n",
    "            \"Recall\": np.mean(scores['test_recall']),\n",
    "            \"F1-score\": np.mean(scores['test_f1']),\n",
    "            \"ROC AUC\": np.mean(scores['test_roc_auc']),\n",
    "        })\n",
    "\n",
    "        print(f\" Modelo {nome} treinado com sucesso.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao rodar o modelo {nome}: {e}\\n\")\n",
    "\n",
    "# Mostra os resultados\n",
    "df_resultados = pd.DataFrame(resultados).sort_values(\"F1-score\", ascending=False)\n",
    "df_resultados"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
