{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30HRRwSCAdKV"
   },
   "source": [
    "## Etapas\n",
    "\n",
    "1. Criação de features\n",
    "2. Tratamento de outliers\n",
    "3. Normalização dos dados\n",
    "4. Separação em treino/teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4F661DIPyJH"
   },
   "outputs": [],
   "source": [
    "# @title Importação de bibliotecas\n",
    "\n",
    "# Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Tratamento de outliers\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 20320,
     "status": "error",
     "timestamp": 1762447144734,
     "user": {
      "displayName": "Pedro",
      "userId": "13797227125436573976"
     },
     "user_tz": 180
    },
    "id": "x-cs77oXAWpf",
    "outputId": "1e7a32a7-0679-4049-a587-db4439c1244b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive not mounted, so nothing to flush and unmount.\n",
      "Mounted at /content/drive\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/QuickAccess/pre-pipeline_wildfires.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3361038527.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Leitura do arquivo .csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mwildfires\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/QuickAccess/pre-pipeline_wildfires.csv'"
     ]
    }
   ],
   "source": [
    "# @title Carregamento do dataset\n",
    "\n",
    "permalink = \"https://github.com/mfigueireddo/ciencia-de-dados/blob/ba579573c5b8a9246ca04f7da29bc2c74c8b362c/datasets/pre-pipeline_wildfires.parquet\"\n",
    "raw_url = permalink.replace(\"/blob/\", \"/raw/\")\n",
    "\n",
    "dest = Path(\"/content/raw_wildfires.parquet\")\n",
    "\n",
    "with requests.get(raw_url, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(dest, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "# Leitura do arquivo .csv\n",
    "wildfires = pd.read_parquet(dest, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUCqwXwyFR2O"
   },
   "outputs": [],
   "source": [
    "# @title 1. Criação de novas features\n",
    "\n",
    "# --- CÉLULA ÚNICA: CRIAÇÃO DE TODAS AS 4 FEATURES ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Verificar se o DataFrame 'wildfires' existe\n",
    "if 'wildfires' not in locals():\n",
    "    print(\"ERRO: O DataFrame 'wildfires' não foi encontrado.\")\n",
    "    print(\"Por favor, execute a Célula 1 (Carregamento do Dataset) primeiro.\")\n",
    "else:\n",
    "    print(\"--- Iniciando a criação de todas as 4 features ---\")\n",
    "\n",
    "    # 2. PREPARAÇÃO: Garantir que 'data' é datetime (necessário para todas)\n",
    "    try:\n",
    "        wildfires['data'] = pd.to_datetime(wildfires['data'])\n",
    "        print(\"Coluna 'data' convertida para datetime.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO crítico ao converter 'data': {e}.\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # FEATURE 1: ESTAÇÃO DO ANO\n",
    "    # ==========================================================\n",
    "    print(\"\\nCriando Feature 1: 'estacao_ano'...\")\n",
    "    try:\n",
    "        if 'estacao_ano' in wildfires.columns:\n",
    "            wildfires = wildfires.drop(columns=['estacao_ano'])\n",
    "\n",
    "        mapa_estacoes = {\n",
    "            12: 'Inverno', 1: 'Inverno', 2: 'Inverno',\n",
    "            3: 'Primavera', 4: 'Primavera', 5: 'Primavera',\n",
    "            6: 'Verão', 7: 'Verão', 8: 'Verão',\n",
    "            9: 'Outono', 10: 'Outono', 11: 'Outono'\n",
    "        }\n",
    "        wildfires['estacao_ano'] = wildfires['data'].dt.month.map(mapa_estacoes)\n",
    "        print(\"✅ Feature 'estacao_ano' criada com sucesso.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO ao criar 'estacao_ano': {e}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # FEATURE 2: REGIÃO DE INCÊNDIO (CLUSTERIZAÇÃO DBSCAN)\n",
    "    # ==========================================================\n",
    "    print(\"\\nCriando Feature 2: 'regiao_incendio'...\")\n",
    "    try:\n",
    "        cols_to_drop = ['lat_cell', 'lon_cell', 'regiao_incendio']\n",
    "        for col in cols_to_drop:\n",
    "            if col in wildfires.columns:\n",
    "                wildfires = wildfires.drop(columns=[col])\n",
    "\n",
    "        DECIMALS = 2\n",
    "        wildfires['lat_cell'] = wildfires['latitude'].round(DECIMALS)\n",
    "        wildfires['lon_cell'] = wildfires['longitude'].round(DECIMALS)\n",
    "\n",
    "        EPS_REFINADO = 0.015  # Raio de ~1.6km\n",
    "        MIN_AMOSTRAS = 5       # Mínimo de 5 células vizinhas\n",
    "        print(f\"Parâmetros do DBSCAN: eps={EPS_REFINADO}, min_samples={MIN_AMOSTRAS}\")\n",
    "\n",
    "        areas_unicas = wildfires[['lat_cell', 'lon_cell']].drop_duplicates().reset_index(drop=True)\n",
    "        print(f\"Encontradas {len(areas_unicas)} células únicas para agrupar.\")\n",
    "\n",
    "        dbscan = DBSCAN(eps=EPS_REFINADO, min_samples=MIN_AMOSTRAS, metric='haversine')\n",
    "        coordenadas_rad = np.radians(areas_unicas[['lat_cell', 'lon_cell']].to_numpy())\n",
    "        clusters = dbscan.fit_predict(coordenadas_rad)\n",
    "\n",
    "        areas_unicas['regiao_incendio'] = clusters\n",
    "        n_regioes = areas_unicas['regiao_incendio'].nunique() - (1 if -1 in clusters else 0)\n",
    "        n_isolados = (areas_unicas['regiao_incendio'] == -1).sum()\n",
    "        print(f\"Resultado: {n_regioes} regiões de incêndio identificadas e {n_isolados} células isoladas.\")\n",
    "\n",
    "        wildfires = wildfires.merge(\n",
    "            areas_unicas[['lat_cell', 'lon_cell', 'regiao_incendio']],\n",
    "            on=['lat_cell', 'lon_cell'],\n",
    "            how='left'\n",
    "        )\n",
    "        print(\"✅ Feature 'regiao_incendio' criada e mapeada de volta ao DataFrame.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO ao criar 'regiao_incendio': {e}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # FEATURES 3 E 4: TENDÊNCIA (MÉDIAS MÓVEIS)\n",
    "    # ==========================================================\n",
    "    print(\"\\nCriando Features 3 & 4: Tendência (Médias Móveis)...\")\n",
    "    try:\n",
    "        # Ordenar os valores (CRUCIAL para cálculos de média móvel)\n",
    "        wildfires = wildfires.sort_values(by=['fire_id', 'data'])\n",
    "        print(\"DataFrame ordenado por 'fire_id' e 'data' (necessário para médias móveis).\")\n",
    "\n",
    "        # Remover colunas antigas se existirem\n",
    "        if 'soma_precipitacao_14d' in wildfires.columns:\n",
    "            wildfires = wildfires.drop(columns=['soma_precipitacao_14d'])\n",
    "        if 'media_temp_max_7d' in wildfires.columns:\n",
    "            wildfires = wildfires.drop(columns=['media_temp_max_7d'])\n",
    "\n",
    "        # Criar as features\n",
    "        print(\"Calculando 'soma_precipitacao_14d'...\")\n",
    "        wildfires['soma_precipitacao_14d'] = wildfires.groupby('fire_id')['precipitacao'].transform(\n",
    "            lambda x: x.rolling(window=14, min_periods=1).sum()\n",
    "        )\n",
    "        print(\"Calculando 'media_temp_max_7d'...\")\n",
    "        wildfires['media_temp_max_7d'] = wildfires.groupby('fire_id')['temperatura_max'].transform(\n",
    "            lambda x: x.rolling(window=7, min_periods=1).mean()\n",
    "        )\n",
    "        print(\"✅ Features de tendência criadas com sucesso!\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"ERRO: Faltando uma coluna necessária: {e}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Um erro inesperado ocorreu: {e}\")\n",
    "\n",
    "    # ==========================================================\n",
    "    # VALIDAÇÃO FINAL CONSOLIDADA\n",
    "    # ==========================================================\n",
    "    print(\"\\n--- Processo completo de criação de features concluído! ---\")\n",
    "    print(\"Amostra do DataFrame com todas as novas features:\")\n",
    "\n",
    "    colunas_finais = [\n",
    "        'data', 'fire_id', 'estacao_ano', 'regiao_incendio',\n",
    "        'soma_precipitacao_14d', 'media_temp_max_7d'\n",
    "    ]\n",
    "    # Apenas para garantir que colunas auxiliares não estejam na validação\n",
    "    colunas_existentes = [col for col in colunas_finais if col in wildfires.columns]\n",
    "\n",
    "    display(wildfires[colunas_existentes].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYTi9RrGQd0x"
   },
   "outputs": [],
   "source": [
    "# @title 2. Tratamento de outliers\n",
    "\n",
    "log_cols = [\n",
    "    \"precipitacao\",\n",
    "    \"velocidade_vento\",\n",
    "    \"evapotranspiracao_potencial\",\n",
    "    \"deficit_pressao_vapor\",\n",
    "]\n",
    "sqrt_cols = [\n",
    "    \"umidade_especifica\",\n",
    "    \"umidade_combustivel_morto_100_horas\",\n",
    "    \"umidade_combustivel_morto_1000_horas\",\n",
    "    \"evapotranspiracao_real\",\n",
    "]\n",
    "winsor_cols = [\n",
    "    \"umidade_relativa_max\",\n",
    "    \"umidade_relativa_min\",\n",
    "    \"temperatura_min\",\n",
    "    \"temperatura_max\",\n",
    "    \"indice_queima\",\n",
    "]\n",
    "\n",
    "class LogSqrtWinsorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, log_cols=None, sqrt_cols=None, winsor_cols=None, winsor_limits=(0.05, 0.05)):\n",
    "        self.log_cols = log_cols or []\n",
    "        self.sqrt_cols = sqrt_cols or []\n",
    "        self.winsor_cols = winsor_cols or []\n",
    "        self.winsor_limits = winsor_limits\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        Xdf = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "\n",
    "        self.log_offset_ = {}\n",
    "        for c in self.log_cols:\n",
    "            if c in Xdf.columns:\n",
    "                s = pd.to_numeric(Xdf[c], errors=\"coerce\")\n",
    "                m = s.min()\n",
    "                self.log_offset_[c] = (abs(m) + 1) if pd.notna(m) and m <= 0 else 1.0\n",
    "\n",
    "        self.sqrt_offset_ = {}\n",
    "        for c in self.sqrt_cols:\n",
    "            if c in Xdf.columns:\n",
    "                s = pd.to_numeric(Xdf[c], errors=\"coerce\")\n",
    "                m = s.min()\n",
    "                self.sqrt_offset_[c] = (abs(m) + 0.01) if pd.notna(m) and m < 0 else 0.0\n",
    "\n",
    "        present_winsor = [c for c in self.winsor_cols if c in Xdf.columns]\n",
    "        if present_winsor:\n",
    "            q_low, q_high = self.winsor_limits\n",
    "            self.low_  = pd.to_numeric(Xdf[present_winsor], errors=\"coerce\").quantile(q_low)\n",
    "            self.high_ = pd.to_numeric(Xdf[present_winsor], errors=\"coerce\").quantile(1 - q_high)\n",
    "        else:\n",
    "            self.low_ = pd.Series(dtype=float)\n",
    "            self.high_ = pd.Series(dtype=float)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Xdf = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X).copy()\n",
    "\n",
    "        # LOG\n",
    "        for c, off in self.log_offset_.items():\n",
    "            if c in Xdf.columns:\n",
    "                s = pd.to_numeric(Xdf[c], errors=\"coerce\")\n",
    "                Xdf[c] = np.log(s + off)\n",
    "\n",
    "        # SQRT\n",
    "        for c, off in self.sqrt_offset_.items():\n",
    "            if c in Xdf.columns:\n",
    "                s = pd.to_numeric(Xdf[c], errors=\"coerce\")\n",
    "                Xdf[c] = np.sqrt(s + off)\n",
    "\n",
    "        # Winsorização\n",
    "        for c in self.low_.index:\n",
    "            if c in Xdf.columns:\n",
    "                s = pd.to_numeric(Xdf[c], errors=\"coerce\")\n",
    "                Xdf[c] = s.clip(lower=self.low_[c], upper=self.high_[c])\n",
    "\n",
    "        return Xdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXWH4spcYhkY"
   },
   "source": [
    "**Transformação Logarítmica**\n",
    "- Aplica log(x) ou log(x+constante) para valores positivos\n",
    "- Muito eficaz para dados com distribuição assimétrica positiva\n",
    "- Comprime valores grandes e expande valores pequenos\n",
    "- Fórmula: X_log = log(X + c), onde c evita log(0)\n",
    "\n",
    "**Transformação Raiz Quadrada**\n",
    "- Menos drástica que a transformação logarítmica\n",
    "- Útil para dados de contagem e variáveis positivamente assimétricas\n",
    "- Fórmula: X_sqrt = sqrt(X)\n",
    "\n",
    "**Winsorização (Capping/Clipping)**\n",
    "\n",
    "A **Winsorização** é uma técnica de tratamento de outliers que **limita valores extremos** sem removê-los completamente. Em vez de excluir outliers, substituímos os valores extremos pelos valores de percentis específicos.\n",
    "\n",
    "**Como funciona:**\n",
    "- Define-se limites baseados em percentis (ex: 5º e 95º percentil)\n",
    "- Valores abaixo do limite inferior são substituídos pelo valor do limite inferior\n",
    "- Valores acima do limite superior são substituídos pelo valor do limite superior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmhrgtvXBUL8"
   },
   "source": [
    "| Variável                                 | Melhor método         | Justificativa (1 linha)                                                                                   |\n",
    "| :--------------------------------------- | :-------------------- | :-------------------------------------------------------------------------------------------------------- |\n",
    "| **precipitacao**                         | **Log(x + 1)**        | Reduziu fortemente a assimetria (7.87 → 2.59) e manteve os limites IQR estáveis — ideal para cauda longa. |\n",
    "| **umidade_relativa_max**                 | **Winsorizar 5%-5%**  | Cortou outliers (23 → 0) e melhorou levemente a simetria; log/sqrt inflaram valores.                      |\n",
    "| **umidade_relativa_min**                 | **Winsorizar 5%-5%**  | Assimetria e outliers foram totalmente corrigidos (1155 → 0).                                             |\n",
    "| **umidade_especifica**                   | **Sqrt**              | Melhor simetria (0.89 → 0.16) e forte redução de outliers (6967 → 2102).                                  |\n",
    "| **radiacao_solar**                       | **Sem transformação** | Já simétrica e sem outliers; log piorou, winsor apenas repete.                                            |\n",
    "| **temperatura_min**                      | **Winsorizar 5%-5%**  | Remoção completa de outliers (5066 → 0) e leve ganho de simetria.                                         |\n",
    "| **temperatura_max**                      | **Winsorizar 5%-5%**  | Mesmo comportamento de `temperatura_min`.                                                                 |\n",
    "| **velocidade_vento**                     | **Log(x + 1)**        | Reduziu assimetria (1.23 → 0.19) e outliers (8723 → 1128) sem eliminar extremos reais.                    |\n",
    "| **indice_queima**                        | **Winsorizar 5%-5%**  | Log e sqrt aumentaram outliers via IQR; winsor eliminou-os com mínima distorção.                          |\n",
    "| **umidade_combustivel_morto_100_horas**  | **Sqrt**              | Forte queda de outliers (44 → 9) e leve suavização de forma.                                              |\n",
    "| **umidade_combustivel_morto_1000_horas** | **Sqrt**              | Reduziu outliers (373 → 4) mantendo distribuição coerente.                                                |\n",
    "| **componente_energia_lancada**           | **Sem transformação** | Já equilibrada; transformações criam falsos outliers.                                                     |\n",
    "| **evapotranspiracao_real**               | **Sqrt**              | Melhorou drasticamente a simetria (0.71 → -0.00) e reduziu outliers (3292 → 153).                         |\n",
    "| **evapotranspiracao_potencial**          | **Log(x + 1)**        | Skew caiu (0.53 → -0.36) e outliers zeraram (679 → 0).                                                    |\n",
    "| **deficit_pressao_vapor**                | **Log(x + 1)**        | Alta cauda direita suavizada (1.46 → 0.52) e outliers despencaram (14758 → 672).                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_HM_loh1QihZ"
   },
   "outputs": [],
   "source": [
    "# @title 3. Normalização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4l1XXAgXQkt7"
   },
   "outputs": [],
   "source": [
    "# @title 4. Separação em treino/teste\n",
    "\n",
    "def group_time_series_cv(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str,\n",
    "    group_col: str,\n",
    "    *,\n",
    "    n_splits: int = 5,\n",
    "    test_groups_size: int = 1,\n",
    "    gap_groups: int = 0,\n",
    "    expanding: bool = True,\n",
    "    min_train_groups: int | None = None,\n",
    "    step_groups: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Gera folds (train_idx, test_idx) respeitando ordem temporal por grupo,\n",
    "    embargo em nível de grupo e exclusão mútua treino/teste por grupo.\n",
    "    \"\"\"\n",
    "    # 1) Ordena grupos pelo primeiro timestamp\n",
    "    first_time = (\n",
    "        df[[group_col, time_col]]\n",
    "        .dropna(subset=[time_col])\n",
    "        .groupby(group_col)[time_col]\n",
    "        .min()\n",
    "        .sort_values()\n",
    "    )\n",
    "    ordered_groups = first_time.index.to_numpy()\n",
    "    n_groups = len(ordered_groups)\n",
    "\n",
    "    if step_groups is None:\n",
    "        step_groups = test_groups_size\n",
    "    if min_train_groups is None:\n",
    "        # mínimo sensato: pelo menos o tamanho do primeiro teste\n",
    "        min_train_groups = max(1, test_groups_size)\n",
    "\n",
    "    # Âncora: último grupo incluso no treino\n",
    "    # Precisamos garantir espaço para gap + teste à frente\n",
    "    max_anchor = n_groups - gap_groups - test_groups_size\n",
    "    if max_anchor <= min_train_groups:\n",
    "        return  # não há splits possíveis\n",
    "\n",
    "    splits = 0\n",
    "    anchor = min_train_groups\n",
    "    while anchor <= max_anchor and splits < n_splits:\n",
    "        if expanding:\n",
    "            train_groups = ordered_groups[:anchor]\n",
    "        else:\n",
    "            start = max(0, anchor - min_train_groups)\n",
    "            train_groups = ordered_groups[start:anchor]\n",
    "\n",
    "        test_start = anchor + gap_groups\n",
    "        test_end = test_start + test_groups_size\n",
    "        test_groups = ordered_groups[test_start:test_end]\n",
    "\n",
    "        train_idx = df.index[df[group_col].isin(train_groups)].to_numpy()\n",
    "        test_idx  = df.index[df[group_col].isin(test_groups)].to_numpy()\n",
    "\n",
    "        if train_idx.size and test_idx.size:\n",
    "            yield (train_idx, test_idx)\n",
    "            splits += 1\n",
    "\n",
    "        anchor += step_groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSxExLXsXrdc"
   },
   "source": [
    "## Time Series Cross Validation\n",
    "\n",
    "A Time Series Cross Validation é uma técnica especializada para validar modelos quando os dados possuem ordem cronológica. Diferente das técnicas tradicionais, ela respeita a estrutura temporal dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyUKC9duPwYA"
   },
   "outputs": [],
   "source": [
    "# @title Pipeline\n",
    "\n",
    "preprocess = Pipeline(steps=[\n",
    "    (\"features\", FunctionTransformer(build_features,  validate=False)),\n",
    "    (\"outliers\", LogSqrtWinsorizer(\n",
    "        log_cols=log_cols,\n",
    "        sqrt_cols=sqrt_cols,\n",
    "        winsor_cols=winsor_cols,\n",
    "        winsor_limits=(0.05, 0.05),    # mantenha seus limites\n",
    "    )),\n",
    "    (\"scaler\",   FunctionTransformer(scale_features,  validate=False)),\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
