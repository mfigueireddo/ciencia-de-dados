{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30HRRwSCAdKV"
   },
   "source": [
    "## Etapas\n",
    "\n",
    "1. Criação de features\n",
    "2. Tratamento de outliers\n",
    "3. Normalização dos dados\n",
    "4. Ajustes nos dados\n",
    "5. Pipeline\n",
    "6. Separação em treino/teste\n",
    "7. Treinando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4F661DIPyJH"
   },
   "outputs": [],
   "source": [
    "# @title Importação de bibliotecas\n",
    "\n",
    "# Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Criação de features\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Tratamento de outliers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Ajustes nos dados\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Treinamento do modelo \n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 20320,
     "status": "error",
     "timestamp": 1762447144734,
     "user": {
      "displayName": "Pedro",
      "userId": "13797227125436573976"
     },
     "user_tz": 180
    },
    "id": "x-cs77oXAWpf",
    "outputId": "1e7a32a7-0679-4049-a587-db4439c1244b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive not mounted, so nothing to flush and unmount.\n",
      "Mounted at /content/drive\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/QuickAccess/pre-pipeline_wildfires.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3361038527.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Leitura do arquivo .csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mwildfires\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/QuickAccess/pre-pipeline_wildfires.csv'"
     ]
    }
   ],
   "source": [
    "# @title Carregamento do dataset\n",
    "\n",
    "permalink = \"https://github.com/mfigueireddo/ciencia-de-dados/blob/ba579573c5b8a9246ca04f7da29bc2c74c8b362c/datasets/pre-pipeline_wildfires.parquet\"\n",
    "raw_url = permalink.replace(\"/blob/\", \"/raw/\")\n",
    "\n",
    "dest = Path(\"/content/raw_wildfires.parquet\")\n",
    "\n",
    "with requests.get(raw_url, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(dest, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "# Leitura do arquivo .csv\n",
    "wildfires = pd.read_parquet(dest, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Criação de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUCqwXwyFR2O"
   },
   "outputs": [],
   "source": [
    "# @title Criação de features\n",
    "\n",
    "class TemporalGeoFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Gera:\n",
    "      - estacao_ano_num  (0=Verão, 1=Outono, 2=Inverno, 3=Primavera)\n",
    "      - regiao_incendio  (cluster DBSCAN atribuído por vizinho-core mais próximo; -1 = ruído/sem cluster)\n",
    "      - soma_precipitacao_14d (rolling por grupo)\n",
    "      - media_temp_max_7d     (rolling por grupo)\n",
    "\n",
    "    Observação: as primeiras linhas de cada grupo podem virar NaN pelas janelas;\n",
    "    trate isso depois na própria Pipeline (ex.: SimpleImputer).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 date_col='data',\n",
    "                 group_col='fire_id',\n",
    "                 lat_col='latitude',\n",
    "                 lon_col='longitude',\n",
    "                 precip_col='precipitacao',\n",
    "                 tmax_col='temp_max',\n",
    "                 window_precip=14,\n",
    "                 window_tmax=7,\n",
    "                 eps_km=1.0,\n",
    "                 min_samples=5):\n",
    "        self.date_col = date_col\n",
    "        self.group_col = group_col\n",
    "        self.lat_col = lat_col\n",
    "        self.lon_col = lon_col\n",
    "        self.precip_col = precip_col\n",
    "        self.tmax_col = tmax_col\n",
    "        self.window_precip = window_precip\n",
    "        self.window_tmax = window_tmax\n",
    "        self.eps_km = eps_km\n",
    "        self.min_samples = min_samples\n",
    "\n",
    "        # Objetos aprendidos no fit\n",
    "        self._dbscan_ = None\n",
    "        self._nn_core_ = None\n",
    "        self._core_labels_ = None\n",
    "        self._eps_rad_ = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_radians(latlon):\n",
    "        return np.radians(latlon.astype(float))\n",
    "\n",
    "    @staticmethod\n",
    "    def _month_to_season_south(month):\n",
    "        # Estações (aprox.) para hemisfério sul:\n",
    "        # Verão:   Dez(12)-Fev(2)  -> 0\n",
    "        # Outono:  Mar(3)-Mai(5)   -> 1\n",
    "        # Inverno: Jun(6)-Ago(8)   -> 2\n",
    "        # Primavera: Set(9)-Nov(11)-> 3\n",
    "        if month in (12, 1, 2):   return 0\n",
    "        if month in (3, 4, 5):    return 1\n",
    "        if month in (6, 7, 8):    return 2\n",
    "        return 3  # 9,10,11\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        missing_cols = [c for c in [self.lat_col, self.lon_col] if c not in X.columns]\n",
    "        if missing_cols:\n",
    "            # Sem lat/lon não há cluster geoespacial — seguimos sem DBSCAN\n",
    "            self._dbscan_ = None\n",
    "            self._nn_core_ = None\n",
    "            self._core_labels_ = None\n",
    "            self._eps_rad_ = None\n",
    "            return self\n",
    "\n",
    "        # DBSCAN em coordenadas (haversine espera radianos)\n",
    "        latlon = X[[self.lat_col, self.lon_col]].to_numpy()\n",
    "        latlon_rad = self._to_radians(latlon)\n",
    "\n",
    "        # converter eps de km para radianos (Raio médio Terra ~ 6371 km)\n",
    "        self._eps_rad_ = self.eps_km / 6371.0\n",
    "\n",
    "        db = DBSCAN(eps=self._eps_rad_, min_samples=self.min_samples, metric='haversine')\n",
    "        db.fit(latlon_rad)\n",
    "        self._dbscan_ = db\n",
    "\n",
    "        # Treina um NN apenas nos pontos-core para atribuição em transform()\n",
    "        core_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        if hasattr(db, 'core_sample_indices_') and len(db.core_sample_indices_) > 0:\n",
    "            core_mask[db.core_sample_indices_] = True\n",
    "            core_points = latlon_rad[core_mask]\n",
    "            core_labels = db.labels_[core_mask]\n",
    "\n",
    "            if len(core_points) > 0:\n",
    "                nn = NearestNeighbors(n_neighbors=1, metric='haversine')\n",
    "                nn.fit(core_points)\n",
    "                self._nn_core_ = nn\n",
    "                self._core_labels_ = core_labels\n",
    "            else:\n",
    "                self._nn_core_ = None\n",
    "                self._core_labels_ = None\n",
    "        else:\n",
    "            self._nn_core_ = None\n",
    "            self._core_labels_ = None\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _assign_dbscan_labels(self, X):\n",
    "        # Se não tivemos lat/lon ou DBSCAN treinado, devolve NaN\n",
    "        if self._nn_core_ is None or self._core_labels_ is None or self._eps_rad_ is None:\n",
    "            return pd.Series([-1] * len(X), index=X.index, dtype='int64')\n",
    "\n",
    "        latlon = X[[self.lat_col, self.lon_col]].to_numpy()\n",
    "        latlon_rad = self._to_radians(latlon)\n",
    "\n",
    "        # Atribui rótulo do core mais próximo, desde que dentro do raio eps\n",
    "        distances, indices = self._nn_core_.kneighbors(latlon_rad, n_neighbors=1, return_distance=True)\n",
    "        distances = distances.reshape(-1)\n",
    "        indices = indices.reshape(-1)\n",
    "\n",
    "        labels = np.full(len(X), -1, dtype='int64')\n",
    "        within = distances <= self._eps_rad_\n",
    "        labels[within] = self._core_labels_[indices[within]]\n",
    "        return pd.Series(labels, index=X.index, dtype='int64')\n",
    "\n",
    "    def _add_temporal_rollings(self, df):\n",
    "        # Ordena por grupo e tempo para garantir rolling correto\n",
    "        if self.group_col in df.columns and self.date_col in df.columns:\n",
    "            df = df.sort_values([self.group_col, self.date_col])\n",
    "        else:\n",
    "            # Se faltar algo, só ordena por data se existir\n",
    "            if self.date_col in df.columns:\n",
    "                df = df.sort_values(self.date_col)\n",
    "\n",
    "        # Rolling de precipitação (soma 14d)\n",
    "        if self.precip_col in df.columns:\n",
    "            df['soma_precipitacao_14d'] = (\n",
    "                df.groupby(self.group_col, dropna=False)[self.precip_col]\n",
    "                  .rolling(self.window_precip, min_periods=1)\n",
    "                  .sum()\n",
    "                  .reset_index(level=0, drop=True)\n",
    "            )\n",
    "        else:\n",
    "            df['soma_precipitacao_14d'] = np.nan\n",
    "\n",
    "        # Rolling de temp máxima (média 7d)\n",
    "        if self.tmax_col in df.columns:\n",
    "            df['media_temp_max_7d'] = (\n",
    "                df.groupby(self.group_col, dropna=False)[self.tmax_col]\n",
    "                  .rolling(self.window_tmax, min_periods=1)\n",
    "                  .mean()\n",
    "                  .reset_index(level=0, drop=True)\n",
    "            )\n",
    "        else:\n",
    "            df['media_temp_max_7d'] = np.nan\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Trabalha em DataFrame para manter nomes/índices\n",
    "        df = pd.DataFrame(X).copy()\n",
    "\n",
    "        # 1) Estação do ano (numérica, 0..3)\n",
    "        if self.date_col in df.columns:\n",
    "            # Garante dtype datetime\n",
    "            df[self.date_col] = pd.to_datetime(df[self.date_col], errors='coerce')\n",
    "            estacao = df[self.date_col].dt.month.map(self._month_to_season_south).astype('Int64')\n",
    "            df['estacao_ano_num'] = estacao.astype('float').astype('Int64')  # evita problemas de NaN -> imputar depois\n",
    "            df['estacao_ano_num'] = df['estacao_ano_num'].astype('float')\n",
    "        else:\n",
    "            df['estacao_ano_num'] = np.nan\n",
    "\n",
    "        # 2) Região por DBSCAN (atribuída por NN para dados novos)\n",
    "        if all(c in df.columns for c in [self.lat_col, self.lon_col]):\n",
    "            df['regiao_incendio'] = self._assign_dbscan_labels(df).astype('int64')\n",
    "        else:\n",
    "            df['regiao_incendio'] = -1\n",
    "\n",
    "        # 3) Rollings temporais por grupo\n",
    "        df = self._add_temporal_rollings(df)\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXWH4spcYhkY"
   },
   "source": [
    "## 2. Tratamento de outliers\n",
    "\n",
    "**Transformação Logarítmica**\n",
    "- Aplica log(x) ou log(x+constante) para valores positivos\n",
    "- Muito eficaz para dados com distribuição assimétrica positiva\n",
    "- Comprime valores grandes e expande valores pequenos\n",
    "- Fórmula: X_log = log(X + c), onde c evita log(0)\n",
    "\n",
    "**Transformação Raiz Quadrada**\n",
    "- Menos drástica que a transformação logarítmica\n",
    "- Útil para dados de contagem e variáveis positivamente assimétricas\n",
    "- Fórmula: X_sqrt = sqrt(X)\n",
    "\n",
    "**Winsorização (Capping/Clipping)**\n",
    "\n",
    "A **Winsorização** é uma técnica de tratamento de outliers que **limita valores extremos** sem removê-los completamente. Em vez de excluir outliers, substituímos os valores extremos pelos valores de percentis específicos.\n",
    "\n",
    "**Como funciona:**\n",
    "- Define-se limites baseados em percentis (ex: 5º e 95º percentil)\n",
    "- Valores abaixo do limite inferior são substituídos pelo valor do limite inferior\n",
    "- Valores acima do limite superior são substituídos pelo valor do limite superior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmhrgtvXBUL8"
   },
   "source": [
    "| Variável                                 | Melhor método         | Justificativa (1 linha)                                                                                   |\n",
    "| :--------------------------------------- | :-------------------- | :-------------------------------------------------------------------------------------------------------- |\n",
    "| **precipitacao**                         | **Log(x + 1)**        | Reduziu fortemente a assimetria (7.87 → 2.59) e manteve os limites IQR estáveis — ideal para cauda longa. |\n",
    "| **umidade_relativa_max**                 | **Winsorizar 5%-5%**  | Cortou outliers (23 → 0) e melhorou levemente a simetria; log/sqrt inflaram valores.                      |\n",
    "| **umidade_relativa_min**                 | **Winsorizar 5%-5%**  | Assimetria e outliers foram totalmente corrigidos (1155 → 0).                                             |\n",
    "| **umidade_especifica**                   | **Sqrt**              | Melhor simetria (0.89 → 0.16) e forte redução de outliers (6967 → 2102).                                  |\n",
    "| **radiacao_solar**                       | **Sem transformação** | Já simétrica e sem outliers; log piorou, winsor apenas repete.                                            |\n",
    "| **temperatura_min**                      | **Winsorizar 5%-5%**  | Remoção completa de outliers (5066 → 0) e leve ganho de simetria.                                         |\n",
    "| **temperatura_max**                      | **Winsorizar 5%-5%**  | Mesmo comportamento de `temperatura_min`.                                                                 |\n",
    "| **velocidade_vento**                     | **Log(x + 1)**        | Reduziu assimetria (1.23 → 0.19) e outliers (8723 → 1128) sem eliminar extremos reais.                    |\n",
    "| **indice_queima**                        | **Winsorizar 5%-5%**  | Log e sqrt aumentaram outliers via IQR; winsor eliminou-os com mínima distorção.                          |\n",
    "| **umidade_combustivel_morto_100_horas**  | **Sqrt**              | Forte queda de outliers (44 → 9) e leve suavização de forma.                                              |\n",
    "| **umidade_combustivel_morto_1000_horas** | **Sqrt**              | Reduziu outliers (373 → 4) mantendo distribuição coerente.                                                |\n",
    "| **componente_energia_lancada**           | **Sem transformação** | Já equilibrada; transformações criam falsos outliers.                                                     |\n",
    "| **evapotranspiracao_real**               | **Sqrt**              | Melhorou drasticamente a simetria (0.71 → -0.00) e reduziu outliers (3292 → 153).                         |\n",
    "| **evapotranspiracao_potencial**          | **Log(x + 1)**        | Skew caiu (0.53 → -0.36) e outliers zeraram (679 → 0).                                                    |\n",
    "| **deficit_pressao_vapor**                | **Log(x + 1)**        | Alta cauda direita suavizada (1.46 → 0.52) e outliers despencaram (14758 → 672).                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYTi9RrGQd0x"
   },
   "outputs": [],
   "source": [
    "# @title 2. Tratamento de outliers\n",
    "\n",
    "log_cols = [\n",
    "    \"precipitacao\",\n",
    "    \"velocidade_vento\",\n",
    "    \"evapotranspiracao_potencial\",\n",
    "    \"deficit_pressao_vapor\",\n",
    "]\n",
    "sqrt_cols = [\n",
    "    \"umidade_especifica\",\n",
    "    \"umidade_combustivel_morto_100_horas\",\n",
    "    \"umidade_combustivel_morto_1000_horas\",\n",
    "    \"evapotranspiracao_real\",\n",
    "]\n",
    "winsor_cols = [\n",
    "    \"umidade_relativa_max\",\n",
    "    \"umidade_relativa_min\",\n",
    "    \"temperatura_min\",\n",
    "    \"temperatura_max\",\n",
    "    \"indice_queima\",\n",
    "]\n",
    "\n",
    "class LogSqrtWinsorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, log_cols=None, sqrt_cols=None, winsor_cols=None, winsor_limits=(0.05, 0.05)):\n",
    "        self.log_cols = log_cols or []\n",
    "        self.sqrt_cols = sqrt_cols or []\n",
    "        self.winsor_cols = winsor_cols or []\n",
    "        self.winsor_limits = winsor_limits\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        Xdf = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "\n",
    "        self.log_offset_ = {}\n",
    "        for c in self.log_cols:\n",
    "            if c in Xdf.columns:\n",
    "                s = pd.to_numeric(Xdf[c], errors=\"coerce\")\n",
    "                m = s.min()\n",
    "                self.log_offset_[c] = (abs(m) + 1) if pd.notna(m) and m <= 0 else 1.0\n",
    "\n",
    "        self.sqrt_offset_ = {}\n",
    "        for c in self.sqrt_cols:\n",
    "            if c in Xdf.columns:\n",
    "                s = pd.to_numeric(Xdf[c], errors=\"coerce\")\n",
    "                m = s.min()\n",
    "                self.sqrt_offset_[c] = (abs(m) + 0.01) if pd.notna(m) and m < 0 else 0.0\n",
    "\n",
    "        present_winsor = [c for c in self.winsor_cols if c in Xdf.columns]\n",
    "        if present_winsor:\n",
    "            q_low, q_high = self.winsor_limits\n",
    "            self.low_  = pd.to_numeric(Xdf[present_winsor], errors=\"coerce\").quantile(q_low)\n",
    "            self.high_ = pd.to_numeric(Xdf[present_winsor], errors=\"coerce\").quantile(1 - q_high)\n",
    "        else:\n",
    "            self.low_ = pd.Series(dtype=float)\n",
    "            self.high_ = pd.Series(dtype=float)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Xdf = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X).copy()\n",
    "\n",
    "        # LOG\n",
    "        for c, off in self.log_offset_.items():\n",
    "            if c in Xdf.columns:\n",
    "                s = pd.to_numeric(Xdf[c], errors=\"coerce\")\n",
    "                Xdf[c] = np.log(s + off)\n",
    "\n",
    "        # SQRT\n",
    "        for c, off in self.sqrt_offset_.items():\n",
    "            if c in Xdf.columns:\n",
    "                s = pd.to_numeric(Xdf[c], errors=\"coerce\")\n",
    "                Xdf[c] = np.sqrt(s + off)\n",
    "\n",
    "        # Winsorização\n",
    "        for c in self.low_.index:\n",
    "            if c in Xdf.columns:\n",
    "                s = pd.to_numeric(Xdf[c], errors=\"coerce\")\n",
    "                Xdf[c] = s.clip(lower=self.low_[c], upper=self.high_[c])\n",
    "\n",
    "        return Xdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_HM_loh1QihZ"
   },
   "outputs": [],
   "source": [
    "# @title 3. Normalização dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ajustes nos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Ajustes nos dados\n",
    "\n",
    "# Colunas categóricas criadas no passo de features\n",
    "_cat_cols = ['estacao_ano_num', 'regiao_incendio']\n",
    "\n",
    "def _numeric_selector(X):\n",
    "    \"\"\"\n",
    "    Seleciona colunas numéricas, exceto as categóricas codificadas numericamente.\n",
    "    (Funciona porque o ColumnTransformer aceita callables como seletores.)\n",
    "    \"\"\"\n",
    "    num = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return [c for c in num if c not in _cat_cols]\n",
    "\n",
    "_numeric_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler',  StandardScaler()),\n",
    "])\n",
    "\n",
    "_categorical_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyUKC9duPwYA"
   },
   "outputs": [],
   "source": [
    "# @title Pipeline\n",
    "\n",
    "preprocess = Pipeline(steps=[\n",
    "    # Criação de features\n",
    "    ('features', TemporalGeoFeatureEngineer(\n",
    "        date_col='data',\n",
    "        group_col='fire_id',\n",
    "        lat_col='latitude',\n",
    "        lon_col='longitude',\n",
    "        precip_col='precipitacao', \n",
    "        tmax_col='temp_max',\n",
    "        window_precip=14,\n",
    "        window_tmax=7,\n",
    "        eps_km=0.96,   # ~0.015 rad * 6371 ≈ 95.6km (compatível com seu eps anterior)\n",
    "        min_samples=5\n",
    "    )),\n",
    "    # Tratamento de outliers\n",
    "    (\"outliers\", LogSqrtWinsorizer(\n",
    "        log_cols=log_cols,\n",
    "        sqrt_cols=sqrt_cols,\n",
    "        winsor_cols=winsor_cols,\n",
    "        winsor_limits=(0.05, 0.05),\n",
    "    )),\n",
    "    # Normalização dos dados\n",
    "    #(\"scaler\",   FunctionTransformer(scale_features,  validate=False)),\n",
    "    # Separação em treino/teste\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSxExLXsXrdc"
   },
   "source": [
    "## 6. Separação em treino/teste\n",
    "\n",
    "### Time Series Cross Validation\n",
    "\n",
    "A Time Series Cross Validation é uma técnica especializada para validar modelos quando os dados possuem ordem cronológica. Diferente das técnicas tradicionais, ela respeita a estrutura temporal dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4l1XXAgXQkt7"
   },
   "outputs": [],
   "source": [
    "# @title 4. Separação em treino/teste\n",
    "\n",
    "def group_time_series_cv(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str,\n",
    "    group_col: str,\n",
    "    *,\n",
    "    n_splits: int = 5,\n",
    "    test_groups_size: int = 1,\n",
    "    gap_groups: int = 0,\n",
    "    expanding: bool = True,\n",
    "    min_train_groups: int | None = None,\n",
    "    step_groups: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Gera folds (train_idx, test_idx) respeitando ordem temporal por grupo,\n",
    "    embargo em nível de grupo e exclusão mútua treino/teste por grupo.\n",
    "    \"\"\"\n",
    "    # 1) Ordena grupos pelo primeiro timestamp\n",
    "    first_time = (\n",
    "        df[[group_col, time_col]]\n",
    "        .dropna(subset=[time_col])\n",
    "        .groupby(group_col)[time_col]\n",
    "        .min()\n",
    "        .sort_values()\n",
    "    )\n",
    "    ordered_groups = first_time.index.to_numpy()\n",
    "    n_groups = len(ordered_groups)\n",
    "\n",
    "    if step_groups is None:\n",
    "        step_groups = test_groups_size\n",
    "    if min_train_groups is None:\n",
    "        # mínimo sensato: pelo menos o tamanho do primeiro teste\n",
    "        min_train_groups = max(1, test_groups_size)\n",
    "\n",
    "    # Âncora: último grupo incluso no treino\n",
    "    # Precisamos garantir espaço para gap + teste à frente\n",
    "    max_anchor = n_groups - gap_groups - test_groups_size\n",
    "    if max_anchor <= min_train_groups:\n",
    "        return  # não há splits possíveis\n",
    "\n",
    "    splits = 0\n",
    "    anchor = min_train_groups\n",
    "    while anchor <= max_anchor and splits < n_splits:\n",
    "        if expanding:\n",
    "            train_groups = ordered_groups[:anchor]\n",
    "        else:\n",
    "            start = max(0, anchor - min_train_groups)\n",
    "            train_groups = ordered_groups[start:anchor]\n",
    "\n",
    "        test_start = anchor + gap_groups\n",
    "        test_end = test_start + test_groups_size\n",
    "        test_groups = ordered_groups[test_start:test_end]\n",
    "\n",
    "        train_idx = df.index[df[group_col].isin(train_groups)].to_numpy()\n",
    "        test_idx  = df.index[df[group_col].isin(test_groups)].to_numpy()\n",
    "\n",
    "        if train_idx.size and test_idx.size:\n",
    "            yield (train_idx, test_idx)\n",
    "            splits += 1\n",
    "\n",
    "        anchor += step_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Executando separação\n",
    "\n",
    "cv_splits = list(group_time_series_cv(\n",
    "    df=wildfires,\n",
    "    time_col='data',\n",
    "    group_col='fire_id',\n",
    "    n_splits=5,\n",
    "    test_groups_size=1,\n",
    "    gap_groups=0,\n",
    "    expanding=True,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Treinando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar X e y (mantém 'data' e 'fire_id' porque o preprocess usa essas colunas)\n",
    "X = wildfires.drop(columns=[\"houve_incendio\"])\n",
    "y = wildfires[\"houve_incendio\"].astype(int)\n",
    "\n",
    "resultados = []\n",
    "\n",
    "modelos = {\n",
    "    \"Dummy (mais frequente)\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"Regressão Logística\": LogisticRegression(max_iter=1000),\n",
    "    \"Árvore de Decisão\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=50, n_jobs=-1),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"SVM (linear)\": SVC(kernel='linear')  # Deixe por último, ele n ta indo (apagar quando for rodar, depois vou tentar arrumar)\n",
    "}\n",
    "\n",
    "# Usar os folds temporais por grupo já definidos anteriormente\n",
    "# (cv_splits veio da célula \"Pipeline\" com group_time_series_cv(...))\n",
    "cv = cv_splits\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': make_scorer(recall_score, zero_division=0),\n",
    "    'f1': make_scorer(f1_score, zero_division=0),\n",
    "    'roc_auc': 'roc_auc'  # deixa o sklearn decidir entre predict_proba/decision_function\n",
    "}\n",
    "\n",
    "for nome, modelo in modelos.items():\n",
    "    print(f\"Treinando modelo: {nome}...\")\n",
    "    try:\n",
    "        pipeline = Pipeline([\n",
    "            (\"preprocess\", preprocess),  # usa a nossa pipeline pronta\n",
    "            (\"clf\", modelo)\n",
    "        ])\n",
    "\n",
    "        scores = cross_validate(pipeline, X, y, cv=cv, scoring=scoring)\n",
    "\n",
    "        resultados.append({\n",
    "            \"Modelo\": nome,\n",
    "            \"Accuracy\": np.mean(scores['test_accuracy']),\n",
    "            \"Precision\": np.mean(scores['test_precision']),\n",
    "            \"Recall\": np.mean(scores['test_recall']),\n",
    "            \"F1-score\": np.mean(scores['test_f1']),\n",
    "            \"ROC AUC\": np.mean(scores['test_roc_auc']),\n",
    "        })\n",
    "\n",
    "        print(f\" Modelo {nome} treinado com sucesso.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao rodar o modelo {nome}: {e}\\n\")\n",
    "\n",
    "# Mostrar resultados\n",
    "df_resultados = pd.DataFrame(resultados).sort_values(\"F1-score\", ascending=False)\n",
    "df_resultados\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
