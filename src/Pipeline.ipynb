{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30HRRwSCAdKV"
   },
   "source": [
    "## Etapas\n",
    "\n",
    "1. Criação de features\n",
    "2. Tratamento de outliers\n",
    "3. Normalização dos dados\n",
    "4. Pipeline de pré-processamento\n",
    "5. Separação em treino/teste\n",
    "7. Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4F661DIPyJH"
   },
   "outputs": [],
   "source": [
    "# @title Importação das bibliotecas utilizadas no programa\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Carregamento do dataset\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Criação de features\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Normalização dos dados\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Treinamento do modelo \n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 20320,
     "status": "error",
     "timestamp": 1762447144734,
     "user": {
      "displayName": "Pedro",
      "userId": "13797227125436573976"
     },
     "user_tz": 180
    },
    "id": "x-cs77oXAWpf",
    "outputId": "1e7a32a7-0679-4049-a587-db4439c1244b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive not mounted, so nothing to flush and unmount.\n",
      "Mounted at /content/drive\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/QuickAccess/pre-pipeline_wildfires.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3361038527.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Leitura do arquivo .csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mwildfires\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/QuickAccess/pre-pipeline_wildfires.csv'"
     ]
    }
   ],
   "source": [
    "# @title Carregamento do dataset\n",
    "\n",
    "github_link = \"https://github.com/mfigueireddo/ciencia-de-dados/blob/ba579573c5b8a9246ca04f7da29bc2c74c8b362c/datasets/pre-pipeline_wildfires.parquet\"\n",
    "url = github_link.replace(\"/blob/\", \"/raw/\")\n",
    "\n",
    "local_file_path = Path(\"/content/raw_wildfires.parquet\")\n",
    "\n",
    "# Faz uma requisição HTTP GET ao GitHub\n",
    "with requests.get(url, stream=True) as request:\n",
    "\n",
    "    request.raise_for_status() # Confere se houve êxito\n",
    "\n",
    "    with open(local_file_path , \"wb\") as file:\n",
    "        for chunk in request.iter_content(chunk_size=1024*1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "\n",
    "wildfires = pd.read_parquet(local_file_path,  engine=\"pyarrow\") # Leitura realizada com a engine pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Variáveis globais\n",
    "\n",
    "data_column_name = 'data'\n",
    "id_column_name = 'fire_id'\n",
    "latitude_column_name = 'latitude'\n",
    "longitude_column_name = 'longitude'\n",
    "precipitation_column_name = 'precipitacao'\n",
    "max_temperature_column_name = 'temperatura_max'\n",
    "precipitation_sum_window_column_name = 'soma_precipitacao_14d'\n",
    "max_temperature_mean_column_name = 'media_temp_max_7d'\n",
    "season_column_name = 'estacao_ano_id'\n",
    "region_column_name = 'regiao_incendio'\n",
    "target_column_name = 'houve_incendio'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Criação de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features geradas\n",
    "- Estação do ano baseada no hemisfério Sul (0=Verão, 1=Outono, 2=Inverno, 3=Primavera)\n",
    "- Região do incêndio\n",
    "- Soma da precipitação nos últimos 14 dias\n",
    "- Média de temperatura máxima nos últimos 7 dias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUCqwXwyFR2O"
   },
   "outputs": [],
   "source": [
    "# @title Classe responsável pela criação de features\n",
    "\n",
    "class FeaturesCreation(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Colunas utilizadas\n",
    "        self.m_date_column = data_column_name\n",
    "        self.m_group_column = id_column_name\n",
    "        self.m_latitude_column = latitude_column_name\n",
    "        self.m_longitude_column = longitude_column_name\n",
    "        self.m_precipitation_column = precipitation_column_name \n",
    "        self.m_max_temperature_column = max_temperature_column_name \n",
    "\n",
    "        # Parâmetros personalizados para criação das features\n",
    "        self.m_precipitation_window_days = 90\n",
    "        self.m_max_temperature_window_days =90\n",
    "\n",
    "        # Parâmetros do DBSCAN\n",
    "        self.mm_max_radiuskm = 1.0\n",
    "        self.m_min_samples = 5\n",
    "\n",
    "        # Objetos aprendidos no fit\n",
    "        self.m_dbscan = None\n",
    "        self.m_nearest_neighbors_core = None\n",
    "        self.m_core_labels = None\n",
    "        self.m_max_radius = None\n",
    "\n",
    "    # Conversão necessária para o DBSCAN\n",
    "    @staticmethod\n",
    "    def convert_to_radians(latitude_or_longitude):\n",
    "        return np.radians(latitude_or_longitude.astype(float))\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_month_to_season(month):\n",
    "        if month in (12, 1, 2): return 0  # Inverno\n",
    "        if month in (3, 4, 5): return 1  # Primavera\n",
    "        if month in (6, 7, 8): return 2  # Verão\n",
    "        return 3  # 9, 10, 11 -> Outono\n",
    "\n",
    "    def fit(self, dataframe, target=None):\n",
    "        dataframe = dataframe.copy()\n",
    "\n",
    "        # Desativa o DBSCAN caso não haja latitude e longitude\n",
    "        missing_cols = [column for column in [self.m_latitude_column, self.m_longitude_column] if column not in dataframe.columns]\n",
    "        if missing_cols:\n",
    "            self.m_dbscan = None\n",
    "            self.m_nearest_neighbors_core = None\n",
    "            self.m_core_labels = None\n",
    "            self.m_max_radius = None\n",
    "            return self\n",
    "\n",
    "        latitude_or_longitude = dataframe[[self.m_latitude_column, self.m_longitude_column]].to_numpy()\n",
    "        latitude_or_longitude_radians = self.convert_to_radians(latitude_or_longitude)\n",
    "\n",
    "        earth_radius = 6371.0\n",
    "        self.m_max_radius = self.mm_max_radiuskm / earth_radius\n",
    "\n",
    "        dbscan = DBSCAN(eps=self.m_max_radius, m_min_samples=self.m_min_samples, metric='haversine')\n",
    "        dbscan.fit(latitude_or_longitude_radians)\n",
    "        self.m_dbscan = dbscan\n",
    "\n",
    "        # Treina um NearestNeighbors apenas nos pontos-core para atribuição de novos pontos à clusters existentes em transform()\n",
    "        core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "        if hasattr(dbscan, 'core_sample_indices_') and len(dbscan.core_sample_indices_) > 0:\n",
    "            core_mask[dbscan.core_sample_indices_] = True\n",
    "            core_points = latitude_or_longitude_radians[core_mask]\n",
    "            core_labels = dbscan.labels_[core_mask]\n",
    "\n",
    "            if len(core_points) > 0:\n",
    "                nearest_neighbors = NearestNeighbors(n_neighbors=1, metric='haversine')\n",
    "                nearest_neighbors.fit(core_points)\n",
    "                self.m_nearest_neighbors_core = nearest_neighbors\n",
    "                self.m_core_labels = core_labels\n",
    "            else:\n",
    "                self.m_nearest_neighbors_core = None\n",
    "                self.m_core_labels = None\n",
    "        else:\n",
    "            self.m_nearest_neighbors_core = None\n",
    "            self.m_core_labels = None\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Rotula novos pontos\n",
    "    def assign_dbscanlabels(self, dataframe):\n",
    "\n",
    "        # Se não tivemos lat/lon ou DBSCAN treinado, devolve NaN\n",
    "        if self.m_nearest_neighbors_core is None or self.m_core_labels is None or self.m_max_radius is None:\n",
    "            return pd.Series([-1] * len(dataframe), index=dataframe.index, dtype='int64')\n",
    "\n",
    "        latitude_or_longitude = dataframe[[self.m_latitude_column, self.m_longitude_column]].to_numpy()\n",
    "        latitude_or_longitude_radians = self.convert_to_radians(latitude_or_longitude)\n",
    "\n",
    "        # Atribui rótulo do core mais próximo, desde que dentro do raio\n",
    "        distances, indices = self.m_nearest_neighbors_core.kneighbors(latitude_or_longitude_radians, n_neighbors=1, return_distance=True)\n",
    "        distances = distances.reshape(-1)\n",
    "        indices = indices.reshape(-1)\n",
    "\n",
    "        labels = np.full(len(dataframe), -1, dtype='int64')\n",
    "        within = distances <= self.m_max_radius\n",
    "        labels[within] = self.m_core_labels[indices[within]]\n",
    "\n",
    "        return pd.Series(labels, index=dataframe.index, dtype='int64')\n",
    "\n",
    "    # Adiciona médias móveis e soma pro grupo de incêndio\n",
    "    def add_temporal_rollings(self, dataframe):\n",
    "        \n",
    "        # Ordena por grupo e tempo para garantir rolling correto\n",
    "        if self.m_group_column in dataframe.columns and self.m_date_column in dataframe.columns:\n",
    "            dataframe = dataframe.sort_values([self.m_group_column, self.m_date_column])\n",
    "        else:\n",
    "            # Se faltar algo, só ordena por data (se existir)\n",
    "            if self.m_date_column in dataframe.columns:\n",
    "                dataframe = dataframe.sort_values(self.m_date_column)\n",
    "\n",
    "        # Rolling de precipitação (soma dos últimos 14 dias)\n",
    "        if self.m_precipitation_column in dataframe.columns:\n",
    "            dataframe[precipitation_sum_window_column_name] = (\n",
    "                dataframe.groupby(self.m_group_column, dropna=False)[self.m_precipitation_column]\n",
    "                  .rolling(self.m_precipitation_window_days, min_periods=1)\n",
    "                  .sum()\n",
    "                  .reset_index(level=0, drop=True)\n",
    "            )\n",
    "        else:\n",
    "            dataframe[precipitation_sum_window_column_name] = np.nan\n",
    "\n",
    "        # Rolling de temperatura máxima (média dos últimos 7 dias)\n",
    "        if self.m_max_temperature_column in dataframe.columns:\n",
    "            dataframe[max_temperature_mean_column_name] = (\n",
    "                dataframe.groupby(self.m_group_column, dropna=False)[self.m_max_temperature_column]\n",
    "                  .rolling(self.m_max_temperature_window_days, min_periods=1)\n",
    "                  .mean()\n",
    "                  .reset_index(level=0, drop=True)\n",
    "            )\n",
    "        else:\n",
    "            dataframe[max_temperature_mean_column_name] = np.nan\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    # Aplica transformações e cria as novas features\n",
    "    def transform(self, dataframe, target=None):\n",
    "        \n",
    "        # Trabalha em DataFrame para manter nomes/índices\n",
    "        dataframe = pd.DataFrame(dataframe).copy()\n",
    "\n",
    "        # Estação do ano\n",
    "        if self.m_date_column in dataframe.columns:\n",
    "            # Garante dtype datetime\n",
    "            dataframe[self.m_date_column] = pd.to_datetime(dataframe[self.m_date_column], errors='coerce')\n",
    "            estacao = dataframe[self.m_date_column].dt.month.map(self.convert_month_to_season).astype('Int64')\n",
    "            dataframe[season_column_name] = estacao.astype('float').astype('Int64')  # evita problemas de NaN -> imputar depois\n",
    "            dataframe[season_column_name] = dataframe[season_column_name].astype('float')\n",
    "        else:\n",
    "            dataframe[season_column_name] = np.nan\n",
    "\n",
    "        # Região \n",
    "        if all(c in dataframe.columns for c in [self.m_latitude_column, self.m_longitude_column]):\n",
    "            dataframe[region_column_name] = self.assign_dbscanlabels(dataframe).astype('int64')\n",
    "        else:\n",
    "            dataframe[region_column_name] = -1\n",
    "\n",
    "        # Rollings temporais\n",
    "        dataframe = self.add_temporal_rollings(dataframe)\n",
    "\n",
    "        return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustes nos dados\n",
    "\n",
    "As features estação do ano e região do incêndio que foram criadas precisam ser tratadas. \n",
    "Isso porque seus valores são categóricos e não há relação numérica entre eles. \n",
    "Isto será feito na normalização dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXWH4spcYhkY"
   },
   "source": [
    "## 2. Tratamento de outliers\n",
    "\n",
    "**Transformação Logarítmica**\n",
    "- Aplica log(x) ou log(x+constante) para valores positivos\n",
    "- Muito eficaz para dados com distribuição assimétrica positiva\n",
    "- Comprime valores grandes e expande valores pequenos\n",
    "- Fórmula: X_log = log(X + c), onde c evita log(0)\n",
    "\n",
    "**Transformação Raiz Quadrada**\n",
    "- Menos drástica que a transformação logarítmica\n",
    "- Útil para dados de contagem e variáveis positivamente assimétricas\n",
    "- Fórmula: X_sqrt = sqrt(X)\n",
    "\n",
    "**Winsorização (Capping/Clipping)**\n",
    "\n",
    "A **Winsorização** é uma técnica de tratamento de outliers que **limita valores extremos** sem removê-los completamente. Em vez de excluir outliers, substituímos os valores extremos pelos valores de percentis específicos.\n",
    "\n",
    "**Como funciona:**\n",
    "- Define-se limites baseados em percentis (ex: 5º e 95º percentil)\n",
    "- Valores abaixo do limite inferior são substituídos pelo valor do limite inferior\n",
    "- Valores acima do limite superior são substituídos pelo valor do limite superior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmhrgtvXBUL8"
   },
   "source": [
    "| Variável                                 | Melhor método         | Justificativa (1 linha)                                                                                   |\n",
    "| :--------------------------------------- | :-------------------- | :-------------------------------------------------------------------------------------------------------- |\n",
    "| **precipitacao**                         | **Log(x + 1)**        | Reduziu fortemente a assimetria (7.87 → 2.59) e manteve os limites IQR estáveis — ideal para cauda longa. |\n",
    "| **umidade_relativa_max**                 | **Winsorizar 5%-5%**  | Cortou outliers (23 → 0) e melhorou levemente a simetria; log/sqrt inflaram valores.                      |\n",
    "| **umidade_relativa_min**                 | **Winsorizar 5%-5%**  | Assimetria e outliers foram totalmente corrigidos (1155 → 0).                                             |\n",
    "| **umidade_especifica**                   | **Sqrt**              | Melhor simetria (0.89 → 0.16) e forte redução de outliers (6967 → 2102).                                  |\n",
    "| **radiacao_solar**                       | **Sem transformação** | Já simétrica e sem outliers; log piorou, winsor apenas repete.                                            |\n",
    "| **temperatura_min**                      | **Winsorizar 5%-5%**  | Remoção completa de outliers (5066 → 0) e leve ganho de simetria.                                         |\n",
    "| **temperatura_max**                      | **Winsorizar 5%-5%**  | Mesmo comportamento de `temperatura_min`.                                                                 |\n",
    "| **velocidade_vento**                     | **Log(x + 1)**        | Reduziu assimetria (1.23 → 0.19) e outliers (8723 → 1128) sem eliminar extremos reais.                    |\n",
    "| **indice_queima**                        | **Winsorizar 5%-5%**  | Log e sqrt aumentaram outliers via IQR; winsor eliminou-os com mínima distorção.                          |\n",
    "| **umidade_combustivel_morto_100_horas**  | **Sqrt**              | Forte queda de outliers (44 → 9) e leve suavização de forma.                                              |\n",
    "| **umidade_combustivel_morto_1000_horas** | **Sqrt**              | Reduziu outliers (373 → 4) mantendo distribuição coerente.                                                |\n",
    "| **componente_energia_lancada**           | **Sem transformação** | Já equilibrada; transformações criam falsos outliers.                                                     |\n",
    "| **evapotranspiracao_real**               | **Sqrt**              | Melhorou drasticamente a simetria (0.71 → -0.00) e reduziu outliers (3292 → 153).                         |\n",
    "| **evapotranspiracao_potencial**          | **Log(x + 1)**        | Skew caiu (0.53 → -0.36) e outliers zeraram (679 → 0).                                                    |\n",
    "| **deficit_pressao_vapor**                | **Log(x + 1)**        | Alta cauda direita suavizada (1.46 → 0.52) e outliers despencaram (14758 → 672).                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYTi9RrGQd0x"
   },
   "outputs": [],
   "source": [
    "# @title Classe responsável pelo tratamento dos outliers\n",
    "\n",
    "log_columns = [\n",
    "    \"precipitacao\",\n",
    "    \"velocidade_vento\",\n",
    "    \"evapotranspiracao_potencial\",\n",
    "    \"deficit_pressao_vapor\",\n",
    "]\n",
    "sqrt_columns = [\n",
    "    \"umidade_especifica\",\n",
    "    \"umidade_combustivel_morto_100_horas\",\n",
    "    \"umidade_combustivel_morto_1000_horas\",\n",
    "    \"evapotranspiracao_real\",\n",
    "]\n",
    "winsor_columns = [\n",
    "    \"umidade_relativa_max\",\n",
    "    \"umidade_relativa_min\",\n",
    "    \"temperatura_min\",\n",
    "    \"temperatura_max\",\n",
    "    \"indice_queima\",\n",
    "]\n",
    "\n",
    "class OutliersTreatment(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.m_log_columns = log_columns \n",
    "        self.m_sqrt_columns = sqrt_columns\n",
    "        self.m_winsor_columns = winsor_columns\n",
    "        self.m_winsor_limits = (0.05, 0.05)\n",
    "\n",
    "    # Calcula parâmetros necessários para aplicar as transformações corretamente\n",
    "    def fit(self, dataframe, target=None):\n",
    "\n",
    "        # Garante que o usuário esteja enviado um dataframe no formato correto\n",
    "        dataframe = dataframe if isinstance(dataframe, pd.DataFrame) else pd.DataFrame(dataframe)\n",
    "\n",
    "        # Cálculo de offsets para garantir que não haverão valores zerados ou negativos\n",
    "        # \"coerce\" convete valores inválidos para NaN\n",
    "\n",
    "        self.m_log_offset = {}\n",
    "        for column in self.m_log_columns:\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                min_value = series.min()\n",
    "                self.m_log_offset[column] = (abs(min_value) + 1) if pd.notna(min_value) and min_value <= 0 else 1.0\n",
    "\n",
    "        self.m_sqrt_offset = {}\n",
    "        for column in self.m_sqrt_columns:\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                min_value = series.min()\n",
    "                self.m_sqrt_offset[column] = (abs(min_value) + 0.01) if pd.notna(min_value) and min_value < 0 else 0.0\n",
    "\n",
    "        # Garante que a winsorização só seja feita com colunas que realmente estão no dataframe\n",
    "        actual_columns_to_winsor = [column for column in self.m_winsor_columns if column in dataframe.columns]\n",
    "        low_quantile, high_quantile = self.m_winsor_limits\n",
    "\n",
    "        if actual_columns_to_winsor:\n",
    "            # Converte cada coluna para numérico (coerces -> NaN) e calcula quantis por coluna\n",
    "            winsor_dataframe = dataframe[actual_columns_to_winsor].apply(pd.to_numeric, errors=\"coerce\")\n",
    "            self.m_low_quantile  = winsor_dataframe.quantile(low_quantile)\n",
    "            self.m_high_quantile = winsor_dataframe.quantile(1 - high_quantile)\n",
    "        else:\n",
    "            # garante atributos vazios para não quebrar no transform()\n",
    "            self.m_low_quantile  = pd.Series(dtype=float)\n",
    "            self.m_high_quantile = pd.Series(dtype=float)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Aplica as transformações\n",
    "    def transform(self, dataframe):\n",
    "\n",
    "        # Garante que o usuário esteja enviado o dataframe correto\n",
    "        dataframe = dataframe.copy() if isinstance(dataframe, pd.DataFrame) else pd.DataFrame(dataframe).copy()\n",
    "\n",
    "        # LOG\n",
    "        for column, offset in self.m_log_offset.items():\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                dataframe[column] = np.log(series + offset)\n",
    "\n",
    "        # SQRT\n",
    "        for column, offset in self.m_sqrt_offset.items():\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                dataframe[column] = np.sqrt(series + offset)\n",
    "\n",
    "        # Winsorização\n",
    "        for column in self.m_low_quantile.index:\n",
    "            if column in dataframe.columns:\n",
    "                series = pd.to_numeric(dataframe[column], errors=\"coerce\")\n",
    "                dataframe[column] = series.clip(lower=self.m_low_quantile[column], upper=self.m_high_quantile[column])\n",
    "\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalização dos dados\n",
    "\n",
    "TO-DO: trazer o algoritmo desenvolvido para normalização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_HM_loh1QihZ"
   },
   "outputs": [],
   "source": [
    "# @title Algoritmo de normalização\n",
    "\n",
    "categoric_cols = [season_column_name, region_column_name] \n",
    "\n",
    "def numeric_columns_selector(dataframe): \n",
    "    # Seleciona colunas numéricas, exceto as categóricas codificadas numericamente\n",
    "    num = dataframe.select_dtypes(include='number').columns.tolist() \n",
    "    return [column for column in num if column not in categoric_cols] \n",
    "\n",
    "numeric_columns_pipeline = Pipeline(steps=[ \n",
    "    ('imputer', SimpleImputer(strategy='median')), \n",
    "    ('scaler', StandardScaler()), \n",
    "    ]) \n",
    "\n",
    "categoric_columns_pipeline = Pipeline(steps=[ \n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), \n",
    "    ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False)), \n",
    "])\n",
    "\n",
    "DataNormalization = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical', numeric_columns_pipeline, numeric_columns_selector),\n",
    "        ('categoric', categoric_columns_pipeline, categoric_cols)\n",
    "    ],\n",
    "    remainder='passthrough'  # mantém quaisquer colunas não listadas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline de pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyUKC9duPwYA"
   },
   "outputs": [],
   "source": [
    "# @title Código da Pipeline de pré-processamento\n",
    "\n",
    "preprocess = Pipeline(steps=[\n",
    "    (\"features_creation\", FeaturesCreation()),\n",
    "    (\"outliers_treatment\", OutliersTreatment()),\n",
    "    (\"data_normalization\", DataNormalization)\n",
    "])\n",
    "\n",
    "# Remove data e fire_id, além de converter o dataframe para o formato esperado pelos classificadores\n",
    "def sanitize_after_preprocess(features):\n",
    "    # Transforma em numpy.ndarray\n",
    "    if isinstance(features, pd.DataFrame):\n",
    "        cols = [column for column in features.columns if column not in ('data', 'fire_id')]\n",
    "        features = features[cols]\n",
    "        features = features.select_dtypes(include='number')\n",
    "    return features\n",
    "\n",
    "sanitize = FunctionTransformer(sanitize_after_preprocess, validate=False, feature_names_out='one-to-one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSxExLXsXrdc"
   },
   "source": [
    "## 5. Separação em treino/teste\n",
    "\n",
    "### Time Series Cross Validation\n",
    "\n",
    "A Time Series Cross Validation é uma técnica especializada para validar modelos quando os dados possuem ordem cronológica. Diferente das técnicas tradicionais, ela respeita a estrutura temporal dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4l1XXAgXQkt7"
   },
   "outputs": [],
   "source": [
    "# @title Algoritmo de separação\n",
    "\n",
    "# Gera folds (train_idx, test_idx) respeitando ordem temporal por grupo, embargo em nível de grupo e exclusão mútua treino/teste por grupo.\n",
    "def group_time_series_cross_validation():\n",
    "    dataframe = wildfires\n",
    "    time_column = data_column_name\n",
    "    group_column = id_column_name\n",
    "    folds_amount = 5\n",
    "    fold_groups_size = 1\n",
    "    gap_between_groups_amount = 0\n",
    "\n",
    "    # Ordena grupos pelo primeiro timestamp\n",
    "    first_time = (\n",
    "        dataframe[[group_column, time_column]]\n",
    "        .dropna(subset=[time_column])\n",
    "        .groupby(group_column)[time_column]\n",
    "        .min()\n",
    "        .sort_values()\n",
    "    )\n",
    "    ordered_groups = first_time.index.to_numpy()\n",
    "    ordered_groups_len = len(ordered_groups)\n",
    "\n",
    "    groups_amount_by_step = fold_groups_size\n",
    "\n",
    "    min_train_groups = max(1, fold_groups_size) # pelo menos 1\n",
    "\n",
    "    # Âncora: último grupo incluso no treino\n",
    "    # Precisamos garantir espaço para gap + teste à frente\n",
    "    max_anchor = ordered_groups_len - gap_between_groups_amount - fold_groups_size\n",
    "    if max_anchor <= min_train_groups:\n",
    "        return  # não há splits possíveis\n",
    "\n",
    "    splits = 0\n",
    "    anchor = min_train_groups\n",
    "    while anchor <= max_anchor and splits < folds_amount:\n",
    "        train_groups = ordered_groups[:anchor]\n",
    "\n",
    "        test_start = anchor + gap_between_groups_amount\n",
    "        test_end = test_start + fold_groups_size\n",
    "        test_groups = ordered_groups[test_start:test_end]\n",
    "\n",
    "        train_idx = dataframe.index[dataframe[group_column].isin(train_groups)].to_numpy()\n",
    "        test_idx  = dataframe.index[dataframe[group_column].isin(test_groups)].to_numpy()\n",
    "\n",
    "        if train_idx.size and test_idx.size:\n",
    "            yield (train_idx, test_idx)\n",
    "            splits += 1\n",
    "\n",
    "        anchor += groups_amount_by_step\n",
    "\n",
    "cross_validation = cross_validation_splits = list(group_time_series_cross_validation())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Escolha dos modelos\n",
    "\n",
    "modelos = {\n",
    "    \"Dummy (mais frequente)\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"Regressão Logística\": LogisticRegression(max_iter=1000),\n",
    "    \"Árvore de Decisão\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=50, n_jobs=-1),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Métricas de avaliação\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, zero_division=0),\n",
    "    'recall': make_scorer(recall_score, zero_division=0),\n",
    "    'f1': make_scorer(f1_score, zero_division=0),\n",
    "    'roc_auc': 'roc_auc' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Algoritmo de treinamento\n",
    "\n",
    "# Separar features e target (mantém 'data' e 'fire_id' porque o preprocess usa essas colunas)\n",
    "features = wildfires.drop(columns=target_column_name)\n",
    "target = wildfires[target_column_name].astype(int)\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for nome, modelo in modelos.items():\n",
    "    print(f\"Treinando modelo: {nome}...\")\n",
    "\n",
    "    try:\n",
    "        pipeline = Pipeline([\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"sanitize\", sanitize),\n",
    "            (\"nan_shield\", SimpleImputer(strategy=\"constant\", fill_value=0.0)), # Blindagem contra NaN\n",
    "            (\"classificator\", modelo)\n",
    "        ])\n",
    "\n",
    "        scores = cross_validate(pipeline, features, target, cv=cross_validation, scoring=scoring)\n",
    "\n",
    "        resultados.append({\n",
    "            \"Modelo\": nome,\n",
    "            \"Accuracy\": np.mean(scores['test_accuracy']),\n",
    "            \"Precision\": np.mean(scores['test_precision']),\n",
    "            \"Recall\": np.mean(scores['test_recall']),\n",
    "            \"F1-score\": np.mean(scores['test_f1']),\n",
    "            \"ROC AUC\": np.mean(scores['test_roc_auc']),\n",
    "        })\n",
    "\n",
    "        print(f\" Modelo {nome} treinado com sucesso.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao rodar o modelo {nome}: {e}\\n\")\n",
    "\n",
    "# Mostra os resultados\n",
    "df_resultados = pd.DataFrame(resultados).sort_values(\"F1-score\", ascending=False)\n",
    "df_resultados"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
